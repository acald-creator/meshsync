apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "9402"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-04-01T04:29:32Z"
    generateName: cert-manager-6dc66985d4-
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.14.4
      helm.sh/chart: cert-manager-v1.14.4
      pod-template-hash: 6dc66985d4
    name: cert-manager-6dc66985d4-8t6l8
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-6dc66985d4
      uid: fee7d62e-8d38-490c-a0b5-8fa8f5656674
    resourceVersion: "46089022"
    uid: 4e4f1aa9-a8d4-4c25-abda-3980fdba71ee
  spec:
    containers:
    - args:
      - --v=2
      - --cluster-resource-namespace=$(POD_NAMESPACE)
      - --leader-election-namespace=kube-system
      - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
      - --max-concurrent-challenges=60
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-controller:v1.14.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          path: /livez
          port: http-healthz
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: cert-manager-controller
      ports:
      - containerPort: 9402
        name: http-metrics
        protocol: TCP
      - containerPort: 9403
        name: http-healthz
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-689d6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: c3-medium-x86-03-meshery
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager
    serviceAccountName: cert-manager
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-689d6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:29:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:29:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:29:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:29:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:29:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6ff3f7117716e55827c182a8e23743a27703fcf9c0df6ff7921d9ef7ce506869
      image: quay.io/jetstack/cert-manager-controller:v1.14.4
      imageID: quay.io/jetstack/cert-manager-controller@sha256:5cffa969fd30ce6a760994d30e7cccb3626abc8015d813de52f8cfa9ff862de9
      lastState: {}
      name: cert-manager-controller
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-01T04:29:33Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.16
    podIPs:
    - ip: 192.168.0.16
    qosClass: BestEffort
    startTime: "2024-04-01T04:29:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-04-01T04:29:32Z"
    generateName: cert-manager-cainjector-c7d4dbdd9-
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.14.4
      helm.sh/chart: cert-manager-v1.14.4
      pod-template-hash: c7d4dbdd9
    name: cert-manager-cainjector-c7d4dbdd9-jlstt
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-cainjector-c7d4dbdd9
      uid: 4ad5f4b4-6ddc-4bad-8208-418435b3b8d7
    resourceVersion: "4476"
    uid: b905aeb8-3633-4ca9-b350-df3a87cdd60e
  spec:
    containers:
    - args:
      - --v=2
      - --leader-election-namespace=kube-system
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
      imagePullPolicy: IfNotPresent
      name: cert-manager-cainjector
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sktmn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: c3-medium-x86-03-meshery
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager-cainjector
    serviceAccountName: cert-manager-cainjector
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-sktmn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:29:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:29:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:29:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:29:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:29:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1c7b0a11d4c38a0fce9c3abf2b69358eae486bf383344a18b730db6bf5e135a4
      image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
      imageID: quay.io/jetstack/cert-manager-cainjector@sha256:30286297e5b4b71a86759d297a8109c6a1649fdc68d28f618d87edf12a2da417
      lastState: {}
      name: cert-manager-cainjector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-01T04:29:33Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.14
    podIPs:
    - ip: 192.168.0.14
    qosClass: BestEffort
    startTime: "2024-04-01T04:29:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-04-01T04:29:32Z"
    generateName: cert-manager-webhook-847d7676c9-
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.14.4
      helm.sh/chart: cert-manager-v1.14.4
      pod-template-hash: 847d7676c9
    name: cert-manager-webhook-847d7676c9-89rtq
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-webhook-847d7676c9
      uid: 9ca18977-52e3-474e-93e7-cbcaba9a13fa
    resourceVersion: "47870864"
    uid: 4b127dff-feeb-4962-81b2-27b87d2a1418
  spec:
    containers:
    - args:
      - --v=2
      - --secure-port=10250
      - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
      - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
      - --dynamic-serving-dns-names=cert-manager-webhook
      - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
      - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-webhook:v1.14.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: 6080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: cert-manager-webhook
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      - containerPort: 6080
        name: healthcheck
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 6080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9657m
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: c3-medium-x86-03-meshery
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager-webhook
    serviceAccountName: cert-manager-webhook
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-9657m
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:29:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:29:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:58Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:58Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:29:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f60e25f3ca9dddc430ade42179a757868cf2264e52682a003fb2fd52df7b8efc
      image: quay.io/jetstack/cert-manager-webhook:v1.14.4
      imageID: quay.io/jetstack/cert-manager-webhook@sha256:11f7e7c462da3c0329e0a1e695a7bd37d6b3c28312d4edd4cc8d36f70ecbfa63
      lastState: {}
      name: cert-manager-webhook
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-01T04:29:33Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.15
    podIPs:
    - ip: 192.168.0.15
    qosClass: BestEffort
    startTime: "2024-04-01T04:29:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      component.meshery.io/id: 0253ba12-434a-4eae-9080-460d1d7894b6
      design.meshery.io/version: 0.0.1
    creationTimestamp: "2025-01-10T16:29:21Z"
    labels:
      design.meshery.io/id: c80a3f4e-e764-40b5-84ab-2c9aa0776ffb
    name: pod-es
    namespace: default
    resourceVersion: "63500379"
    uid: be0469ef-d5b1-422d-b489-901f12f3d1b4
  spec:
    containers:
    - image: nginx/nginx
      imagePullPolicy: Always
      name: test
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dd4bf
        readOnly: true
    - image: nginx.io
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dd4bf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-dd4bf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-10T16:29:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-10T16:29:21Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-10T16:29:21Z"
      message: 'containers with unready status: [test nginx]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-10T16:29:21Z"
      message: 'containers with unready status: [test nginx]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-10T16:29:21Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: nginx.io
      imageID: ""
      lastState: {}
      name: nginx
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          message: Back-off pulling image "nginx.io"
          reason: ImagePullBackOff
    - image: nginx/nginx
      imageID: ""
      lastState: {}
      name: test
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          message: Back-off pulling image "nginx/nginx"
          reason: ImagePullBackOff
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Pending
    podIP: 192.168.0.27
    podIPs:
    - ip: 192.168.0.27
    qosClass: BestEffort
    startTime: "2025-01-10T16:29:21Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-10-01T15:29:56Z"
    generateName: emissary-apiext-7d468cdc87-
    labels:
      app.kubernetes.io/instance: emissary-apiext
      app.kubernetes.io/managed-by: kubectl_apply_-f_aes-apiext.yaml
      app.kubernetes.io/name: emissary-apiext
      app.kubernetes.io/part-of: emissary-apiext
      pod-template-hash: 7d468cdc87
    name: emissary-apiext-7d468cdc87-ghrvn
    namespace: emissary-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: emissary-apiext-7d468cdc87
      uid: f02169bb-21b6-4395-bebe-002b8cf4509c
    resourceVersion: "47870822"
    uid: d224ba3a-7783-4906-a1ef-3c54e4599d49
  spec:
    containers:
    - args:
      - --crd-label-selector
      - app.kubernetes.io/part-of=emissary-apiext
      command:
      - apiext
      - emissary-apiext
      image: docker.io/datawire/aes:3.11.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /probes/live
          port: 8080
          scheme: HTTP
        periodSeconds: 3
        successThreshold: 1
        timeoutSeconds: 1
      name: emissary-apiext
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 8443
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /probes/ready
          port: 8080
          scheme: HTTP
        periodSeconds: 3
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      startupProbe:
        failureThreshold: 10
        httpGet:
          path: /probes/live
          port: 8080
          scheme: HTTP
        periodSeconds: 3
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zxzgm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: emissary-apiext
    serviceAccountName: emissary-apiext
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-zxzgm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-10-01T15:29:57Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-10-01T15:29:56Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-10-01T15:29:56Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4d6abc6d01e36706ad17263657daf6d3eea7afe31d26501d392db74316aac9c9
      image: docker.io/datawire/aes:3.11.1
      imageID: docker.io/datawire/aes@sha256:95ec30b3c73256006896daa426d3cb0a21ca08e8cbb84244145140006242bbdb
      lastState: {}
      name: emissary-apiext
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-10-01T15:29:57Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.20
    podIPs:
    - ip: 192.168.0.20
    qosClass: BestEffort
    startTime: "2024-10-01T15:29:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      sidecar.istio.io/inject: "false"
    creationTimestamp: "2024-11-27T01:51:52Z"
    generateName: cm-acme-http-solver-
    labels:
      acme.cert-manager.io/http-domain: "2939030417"
      acme.cert-manager.io/http-token: "1249119978"
      acme.cert-manager.io/http01-solver: "true"
    name: cm-acme-http-solver-s4qmc
    namespace: emissary
    ownerReferences:
    - apiVersion: acme.cert-manager.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: Challenge
      name: certs-staging-playground-5-1341959213-421573756
      uid: dc39fecd-c983-4f45-9aec-a7803811d7e4
    resourceVersion: "53251993"
    uid: c8bb28bf-0b8a-4cd2-aa46-a3df14ce2cf7
  spec:
    automountServiceAccountToken: false
    containers:
    - args:
      - --listen-port=8089
      - --domain=staging-playground.meshery.io
      - --token=Ipel2z0u4V_IALhnKzhhYv_Ko3L_amacLTDL_CyLQKY
      - --key=Ipel2z0u4V_IALhnKzhhYv_Ko3L_amacLTDL_CyLQKY.ODvwH1EwJZU7B64Id9Z2YiGVrUEUhMJcCTqROrNpUrk
      image: quay.io/jetstack/cert-manager-acmesolver:v1.14.4
      imagePullPolicy: IfNotPresent
      name: acmesolver
      ports:
      - containerPort: 8089
        name: http
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 64Mi
        requests:
          cpu: 10m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: c3-medium-x86-03-meshery
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-27T01:51:54Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-27T01:51:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-27T01:51:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-27T01:51:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-27T01:51:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://eb8096544a8a78b71ae3287ae11252fa7ea1025ec418692934ef89ef6943a27b
      image: quay.io/jetstack/cert-manager-acmesolver:v1.14.4
      imageID: quay.io/jetstack/cert-manager-acmesolver@sha256:83aade427ff5d338380aa0b5d0e65a9f1ef9db09215311eb64b94f833e75b864
      lastState: {}
      name: acmesolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-27T01:51:53Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.6
    podIPs:
    - ip: 192.168.0.6
    qosClass: Burstable
    startTime: "2024-11-27T01:51:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    creationTimestamp: "2024-04-01T04:11:02Z"
    generateName: emissary-ingress-5fccb6758d-
    labels:
      app.kubernetes.io/instance: emissary-ingress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: emissary-ingress
      app.kubernetes.io/part-of: emissary-ingress
      helm.sh/chart: emissary-ingress-8.9.1
      pod-template-hash: 5fccb6758d
      product: aes
      profile: main
    name: emissary-ingress-5fccb6758d-2t6sh
    namespace: emissary
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: emissary-ingress-5fccb6758d
      uid: d56ae810-81d2-43b7-a8f8-422777cfaf9d
    resourceVersion: "47870709"
    uid: e7d3efdf-0853-49dc-9a1a-f45e92f0e3aa
  spec:
    containers:
    - env:
      - name: AMBASSADOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: AGENT_CONFIG_RESOURCE_NAME
        value: emissary-ingress-agent-cloud-token
      image: docker.io/emissaryingress/emissary:3.9.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ambassador/v0/check_alive
          port: admin
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 3
        successThreshold: 1
        timeoutSeconds: 1
      name: emissary-ingress
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 8443
        name: https
        protocol: TCP
      - containerPort: 8877
        name: admin
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ambassador/v0/check_ready
          port: admin
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 3
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 600Mi
        requests:
          cpu: 200m
          memory: 300Mi
      securityContext:
        allowPrivilegeEscalation: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/ambassador-pod-info
        name: ambassador-pod-info
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-frqqj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - |
        deployment_name="emissary-apiext"
        deployment_namespace="emissary-system"
        while true; do
          echo "checking if deployment/$deployment_name in namespace: $deployment_namespace exists."
          if kubectl get deployment "$deployment_name" -n $deployment_namespace > /dev/null 2>&1; then
            echo "$deployment_name.$deployment_namespace exists."
            echo "checking if $deployment_name.$deployment_namespace is fully available..."
            kubectl wait --for=condition=available deployment/"$deployment_name" -n $deployment_namespace --timeout=5m
            if [ $? -eq 0 ]; then
              echo "$deployment_name.$deployment_namespace is available"
              while true; do
              desired_replicas=$(kubectl get deployment $deployment_name -n $deployment_namespace -o jsonpath='{.spec.replicas}')
              current_replicas=$(kubectl get deployment $deployment_name -n $deployment_namespace -o jsonpath='{.status.replicas}')
              if [[ $current_replicas != $desired_replicas ]]; then
                echo "$deployment_name.$deployment_namespace is in the process of restarting. Have: $current_replicas, want $desired_replicas"
                sleep 3
              else
                echo "$deployment_name.$deployment_namespace is fully ready and not currently restarting.  Have: $current_replicas, want $desired_replicas"
                break
              fi
              done
              break
            else
              echo "$deployment_name.$deployment_namespace did not become available within the timeout"
            fi
          else
            echo "$deployment_name.$deployment_namespace does not exist yet. Waiting..."
            sleep 3
          fi
        done
      command:
      - /bin/sh
      - -c
      image: istio/kubectl:1.5.10
      imagePullPolicy: IfNotPresent
      name: wait-for-apiext
      resources: {}
      securityContext:
        runAsUser: 8888
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-frqqj
        readOnly: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsUser: 8888
    serviceAccount: emissary-ingress
    serviceAccountName: emissary-ingress
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - downwardAPI:
        defaultMode: 420
        items:
        - fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels
          path: labels
      name: ambassador-pod-info
    - name: kube-api-access-frqqj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:11:03Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:11:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:11:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://310fd1fe7deaadaeb6eb7719c5d26947f901771a16b1f0c56e5df0fb9fe23c12
      image: docker.io/emissaryingress/emissary:3.9.1
      imageID: docker.io/emissaryingress/emissary@sha256:17374fc250f8194a156dd16db9b58359cd170b5e18de6dc213b4830f47f58ce8
      lastState: {}
      name: emissary-ingress
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-01T04:11:04Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    initContainerStatuses:
    - containerID: containerd://c2db955aa10edfd1aa0f05309e9807433652b771e91c1e816c259cc1b9912fb0
      image: docker.io/istio/kubectl:1.5.10
      imageID: docker.io/istio/kubectl@sha256:dbb7726d1bf0229053ebeaf62be3a07025d61b5f4ccc9cef94e0bd9416ddd4cd
      lastState: {}
      name: wait-for-apiext
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://c2db955aa10edfd1aa0f05309e9807433652b771e91c1e816c259cc1b9912fb0
          exitCode: 0
          finishedAt: "2024-04-01T04:11:03Z"
          reason: Completed
          startedAt: "2024-04-01T04:11:03Z"
    phase: Running
    podIP: 192.168.0.12
    podIPs:
    - ip: 192.168.0.12
    qosClass: Burstable
    startTime: "2024-04-01T04:11:02Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-04-01T04:11:02Z"
    generateName: emissary-ingress-agent-7f6bb847d8-
    labels:
      app.kubernetes.io/instance: emissary-ingress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: emissary-ingress-agent
      app.kubernetes.io/part-of: emissary-ingress
      helm.sh/chart: emissary-ingress-8.9.1
      pod-template-hash: 7f6bb847d8
      product: aes
    name: emissary-ingress-agent-7f6bb847d8-k5zk8
    namespace: emissary
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: emissary-ingress-agent-7f6bb847d8
      uid: 27335631-68a8-4de1-887b-037f6762a494
    resourceVersion: "46089061"
    uid: 40324e94-f285-4891-9b41-d4825f056690
  spec:
    containers:
    - env:
      - name: AGENT_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: AGENT_CONFIG_RESOURCE_NAME
        value: emissary-ingress-agent-cloud-token
      - name: RPC_CONNECTION_ADDRESS
        value: https://app.getambassador.io/
      - name: AES_SNAPSHOT_URL
        value: http://emissary-ingress-admin.emissary:8005/snapshot-external
      - name: AES_REPORT_DIAGNOSTICS_TO_CLOUD
        value: "true"
      - name: AES_DIAGNOSTICS_URL
        value: http://emissary-ingress-admin.emissary:8877/ambassador/v0/diag/?json=true
      image: docker.io/ambassador/ambassador-agent:1.0.14
      imagePullPolicy: IfNotPresent
      name: agent
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5d2g6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: emissary-ingress-agent
    serviceAccountName: emissary-ingress-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-5d2g6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:11:03Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:11:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:11:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:11:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:11:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c237c7ab2d696f4eb61179e1d1237c1dec8ba4f84ecebc053bb6cf42c3513e61
      image: docker.io/ambassador/ambassador-agent:1.0.14
      imageID: docker.io/ambassador/ambassador-agent@sha256:23379686e04f803d1a21afc5b0013133216b2f4659868b9a381e38373026eda2
      lastState: {}
      name: agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-01T04:11:03Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.9
    podIPs:
    - ip: 192.168.0.9
    qosClass: BestEffort
    startTime: "2024-04-01T04:11:02Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-11-09T04:07:12Z"
    generateName: ingress-nginx-admission-create-
    labels:
      app.kubernetes.io/component: admission-webhook
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.8.1
      batch.kubernetes.io/controller-uid: 4ae861ee-798b-4b45-92f3-82fad510c9eb
      batch.kubernetes.io/job-name: ingress-nginx-admission-create
      controller-uid: 4ae861ee-798b-4b45-92f3-82fad510c9eb
      job-name: ingress-nginx-admission-create
    name: ingress-nginx-admission-create-zx2dc
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: ingress-nginx-admission-create
      uid: 4ae861ee-798b-4b45-92f3-82fad510c9eb
    resourceVersion: "48624604"
    uid: 330fa077-863b-48b5-bbcd-04fbb185a82d
  spec:
    containers:
    - args:
      - create
      - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
      - --namespace=$(POD_NAMESPACE)
      - --secret-name=ingress-nginx-admission
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20230407@sha256:543c40fd093964bc9ab509d3e791f9989963021f1e9e4c9c7b6700b02bfb227b
      imagePullPolicy: IfNotPresent
      name: create
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kdp5z
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsNonRoot: true
      runAsUser: 2000
    serviceAccount: ingress-nginx-admission
    serviceAccountName: ingress-nginx-admission
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-kdp5z
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-09T04:07:17Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-09T04:07:12Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-09T04:07:15Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-09T04:07:15Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-09T04:07:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://47f83148e7edd5bc8864a3ab9e6a2bcf6032dcaa2c3c2212fb883bd4ab2d10a5
      image: sha256:7e7451bb70423d31bdadcf0a71a3107b64858eccd7827d066234650b5e7b36b0
      imageID: registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:543c40fd093964bc9ab509d3e791f9989963021f1e9e4c9c7b6700b02bfb227b
      lastState: {}
      name: create
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://47f83148e7edd5bc8864a3ab9e6a2bcf6032dcaa2c3c2212fb883bd4ab2d10a5
          exitCode: 0
          finishedAt: "2024-11-09T04:07:14Z"
          reason: Completed
          startedAt: "2024-11-09T04:07:14Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Succeeded
    podIP: 192.168.0.30
    podIPs:
    - ip: 192.168.0.30
    qosClass: BestEffort
    startTime: "2024-11-09T04:07:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-11-09T04:07:12Z"
    generateName: ingress-nginx-admission-patch-
    labels:
      app.kubernetes.io/component: admission-webhook
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.8.1
      batch.kubernetes.io/controller-uid: c3aef92d-c025-42d5-bed8-a58d4dbe7e6b
      batch.kubernetes.io/job-name: ingress-nginx-admission-patch
      controller-uid: c3aef92d-c025-42d5-bed8-a58d4dbe7e6b
      job-name: ingress-nginx-admission-patch
    name: ingress-nginx-admission-patch-n6s9c
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: ingress-nginx-admission-patch
      uid: c3aef92d-c025-42d5-bed8-a58d4dbe7e6b
    resourceVersion: "48624598"
    uid: 620ce2e7-7df0-4b35-a4df-023e0b076063
  spec:
    containers:
    - args:
      - patch
      - --webhook-name=ingress-nginx-admission
      - --namespace=$(POD_NAMESPACE)
      - --patch-mutating=false
      - --secret-name=ingress-nginx-admission
      - --patch-failure-policy=Fail
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20230407@sha256:543c40fd093964bc9ab509d3e791f9989963021f1e9e4c9c7b6700b02bfb227b
      imagePullPolicy: IfNotPresent
      name: patch
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8h6wh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsNonRoot: true
      runAsUser: 2000
    serviceAccount: ingress-nginx-admission
    serviceAccountName: ingress-nginx-admission
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-8h6wh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-09T04:07:16Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-09T04:07:12Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-09T04:07:12Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-09T04:07:12Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-09T04:07:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e6510bf65c5e0fd00f0eb39bfc588d7a40eaeb3190cd453f8067a38b68a00ca6
      image: sha256:7e7451bb70423d31bdadcf0a71a3107b64858eccd7827d066234650b5e7b36b0
      imageID: registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:543c40fd093964bc9ab509d3e791f9989963021f1e9e4c9c7b6700b02bfb227b
      lastState: {}
      name: patch
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://e6510bf65c5e0fd00f0eb39bfc588d7a40eaeb3190cd453f8067a38b68a00ca6
          exitCode: 0
          finishedAt: "2024-11-09T04:07:15Z"
          reason: Completed
          startedAt: "2024-11-09T04:07:15Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Succeeded
    podIP: 192.168.0.32
    podIPs:
    - ip: 192.168.0.32
    qosClass: BestEffort
    startTime: "2024-11-09T04:07:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-11-09T04:07:12Z"
    generateName: ingress-nginx-controller-568fb54f96-
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.8.1
      pod-template-hash: 568fb54f96
    name: ingress-nginx-controller-568fb54f96-w4qt9
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: ingress-nginx-controller-568fb54f96
      uid: 49558274-890d-4bf5-bbb7-a5e0c4bc3db7
    resourceVersion: "48624676"
    uid: b2ffa6e2-412c-4cf5-b16d-a6c94326b506
  spec:
    containers:
    - args:
      - /nginx-ingress-controller
      - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
      - --election-id=ingress-nginx-leader
      - --controller-class=k8s.io/ingress-nginx
      - --ingress-class=nginx
      - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
      - --validating-webhook=:8443
      - --validating-webhook-certificate=/usr/local/certificates/cert
      - --validating-webhook-key=/usr/local/certificates/key
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: LD_PRELOAD
        value: /usr/local/lib/libmimalloc.so
      image: registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /wait-shutdown
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: controller
      ports:
      - containerPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        name: https
        protocol: TCP
      - containerPort: 8443
        name: webhook
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 90Mi
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        runAsUser: 101
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/local/certificates/
        name: webhook-cert
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cd5jd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: ingress-nginx
    serviceAccountName: ingress-nginx
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: webhook-cert
      secret:
        defaultMode: 420
        secretName: ingress-nginx-admission
    - name: kube-api-access-cd5jd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-09T04:07:20Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-09T04:07:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-09T04:07:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-09T04:07:36Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-09T04:07:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://969b1570530d47c51226038aefc3828f43ac43141fb712c3be4b66933389af41
      image: sha256:825aff16c20cc2c6039fce49bafaa0f510de0f9238da475f3de949adadb9be7f
      imageID: registry.k8s.io/ingress-nginx/controller@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd
      lastState: {}
      name: controller
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-09T04:07:20Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.33
    podIPs:
    - ip: 192.168.0.33
    qosClass: Burstable
    startTime: "2024-11-09T04:07:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-11-06T05:51:20Z"
    generateName: coredns-76f75df574-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 76f75df574
    name: coredns-76f75df574-8jzp5
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-76f75df574
      uid: d3ef6fa9-febf-4972-ab5d-d1cf83b38b18
    resourceVersion: "47871071"
    uid: bc8c4ffd-8db5-44e9-911c-e3664b42be52
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kwcfc
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-kwcfc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:51:21Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:51:20Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:51:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:51:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:51:20Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4d3cb49c2b33a100cd7a149d8c8183e6d7dbb0055afa0391473b6d5b8489154c
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imageID: registry.k8s.io/coredns/coredns@sha256:1eeb4c7316bacb1d4c8ead65571cd92dd21e27359f0d4917f1a5822a73b75db1
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-06T05:51:21Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.4
    podIPs:
    - ip: 192.168.0.4
    qosClass: Burstable
    startTime: "2024-11-06T05:51:20Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-11-06T05:51:16Z"
    generateName: coredns-76f75df574-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 76f75df574
    name: coredns-76f75df574-ws7lc
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-76f75df574
      uid: d3ef6fa9-febf-4972-ab5d-d1cf83b38b18
    resourceVersion: "47871031"
    uid: 48c87cbb-6e58-4325-bde0-395549d537e0
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tfdpp
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-tfdpp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:51:17Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:51:16Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:51:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:51:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:51:16Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://11ed66da236799f616ec53fba97c6adc08ac954274c25b5980e5a02feb328d3f
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imageID: registry.k8s.io/coredns/coredns@sha256:1eeb4c7316bacb1d4c8ead65571cd92dd21e27359f0d4917f1a5822a73b75db1
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-06T05:51:17Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.3
    podIPs:
    - ip: 192.168.0.3
    qosClass: Burstable
    startTime: "2024-11-06T05:51:16Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubeadm.kubernetes.io/etcd.advertise-client-urls: https://10.65.18.5:2379
      kubernetes.io/config.hash: 955287bf933cf05484fc77fae916732f
      kubernetes.io/config.mirror: 955287bf933cf05484fc77fae916732f
      kubernetes.io/config.seen: "2024-04-01T03:46:15.019054642Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2024-04-01T03:46:18Z"
    labels:
      component: etcd
      tier: control-plane
    name: etcd-c3-medium-x86-03-meshery
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: c3-medium-x86-03-meshery
      uid: 091022f9-bbba-4427-ada7-00e6e0deb886
    resourceVersion: "47870880"
    uid: ca0e8af2-60e8-4a46-a9f6-3c4c9db56a08
  spec:
    containers:
    - command:
      - etcd
      - --advertise-client-urls=https://10.65.18.5:2379
      - --cert-file=/etc/kubernetes/pki/etcd/server.crt
      - --client-cert-auth=true
      - --data-dir=/var/lib/etcd
      - --experimental-initial-corrupt-check=true
      - --experimental-watch-progress-notify-interval=5s
      - --initial-advertise-peer-urls=https://10.65.18.5:2380
      - --initial-cluster=c3-medium-x86-03-meshery=https://10.65.18.5:2380
      - --key-file=/etc/kubernetes/pki/etcd/server.key
      - --listen-client-urls=https://127.0.0.1:2379,https://10.65.18.5:2379
      - --listen-metrics-urls=http://127.0.0.1:2381
      - --listen-peer-urls=https://10.65.18.5:2380
      - --name=c3-medium-x86-03-meshery
      - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      - --peer-client-cert-auth=true
      - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      - --snapshot-count=10000
      - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      image: registry.k8s.io/etcd:3.5.12-0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /health?exclude=NOSPACE&serializable=true
          port: 2381
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: etcd
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /health?serializable=false
          port: 2381
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/etcd
        name: etcd-data
      - mountPath: /etc/kubernetes/pki/etcd
        name: etcd-certs
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/pki/etcd
        type: DirectoryOrCreate
      name: etcd-certs
    - hostPath:
        path: /var/lib/etcd
        type: DirectoryOrCreate
      name: etcd-data
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:52Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:59Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:59Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4dc1ed9f096c6d3fdee0fff4ea64ac51770d4dd122ff28c31ba44bece915a8d0
      image: registry.k8s.io/etcd:3.5.12-0
      imageID: registry.k8s.io/etcd@sha256:44a8e24dcbba3470ee1fee21d5e88d128c936e9b55d4bc51fbef8086f8ed123b
      lastState: {}
      name: etcd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-01T03:46:15Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 139.178.83.85
    podIPs:
    - ip: 139.178.83.85
    qosClass: Burstable
    startTime: "2024-11-06T05:50:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.65.18.5:6443
      kubernetes.io/config.hash: 4d2df7298a3f73eacea0b299825c7b7c
      kubernetes.io/config.mirror: 4d2df7298a3f73eacea0b299825c7b7c
      kubernetes.io/config.seen: "2024-04-01T03:46:19.804617548Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2024-04-01T03:46:19Z"
    labels:
      component: kube-apiserver
      tier: control-plane
    name: kube-apiserver-c3-medium-x86-03-meshery
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: c3-medium-x86-03-meshery
      uid: 091022f9-bbba-4427-ada7-00e6e0deb886
    resourceVersion: "47870831"
    uid: 572a6dff-c00d-46f3-b02b-c7c818a0f805
  spec:
    containers:
    - command:
      - kube-apiserver
      - --advertise-address=10.65.18.5
      - --allow-privileged=true
      - --authorization-mode=Node,RBAC
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --enable-admission-plugins=NodeRestriction
      - --enable-bootstrap-token-auth=true
      - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      - --etcd-servers=https://127.0.0.1:2379
      - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      - --requestheader-allowed-names=front-proxy-client
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --requestheader-extra-headers-prefix=X-Remote-Extra-
      - --requestheader-group-headers=X-Remote-Group
      - --requestheader-username-headers=X-Remote-User
      - --secure-port=6443
      - --service-account-issuer=https://kubernetes.default.svc.cluster.local
      - --service-account-key-file=/etc/kubernetes/pki/sa.pub
      - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      - --service-cluster-ip-range=10.96.0.0/12
      - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
      image: registry.k8s.io/kube-apiserver:v1.29.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 10.65.18.5
          path: /livez
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-apiserver
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 10.65.18.5
          path: /readyz
          port: 6443
          scheme: HTTPS
        periodSeconds: 1
        successThreshold: 1
        timeoutSeconds: 15
      resources:
        requests:
          cpu: 250m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 10.65.18.5
          path: /livez
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /usr/local/share/ca-certificates
        name: usr-local-share-ca-certificates
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-share-ca-certificates
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /usr/local/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-local-share-ca-certificates
    - hostPath:
        path: /usr/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-share-ca-certificates
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:52Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://53e05dfdf821fbc4696055c3c7b883e65de1eaca2e709b889a301985f934c973
      image: registry.k8s.io/kube-apiserver:v1.29.3
      imageID: registry.k8s.io/kube-apiserver@sha256:ebd35bc7ef24672c5c50ffccb21f71307a82d4fb20c0ecb6d3d27b28b69e0e3c
      lastState: {}
      name: kube-apiserver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-01T03:46:15Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 139.178.83.85
    podIPs:
    - ip: 139.178.83.85
    qosClass: Burstable
    startTime: "2024-11-06T05:50:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: c24568d59745185f80c6701ae6d46b9a
      kubernetes.io/config.mirror: c24568d59745185f80c6701ae6d46b9a
      kubernetes.io/config.seen: "2024-04-01T03:46:19.804619061Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2024-04-01T03:46:19Z"
    labels:
      component: kube-controller-manager
      tier: control-plane
    name: kube-controller-manager-c3-medium-x86-03-meshery
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: c3-medium-x86-03-meshery
      uid: 091022f9-bbba-4427-ada7-00e6e0deb886
    resourceVersion: "47870821"
    uid: 14ea382a-d9bb-45b0-ad37-b142e7b3e3e2
  spec:
    containers:
    - command:
      - kube-controller-manager
      - --allocate-node-cidrs=true
      - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      - --bind-address=127.0.0.1
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --cluster-cidr=10.244.0.0/16
      - --cluster-name=kubernetes
      - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      - --controllers=*,bootstrapsigner,tokencleaner
      - --kubeconfig=/etc/kubernetes/controller-manager.conf
      - --leader-elect=true
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --root-ca-file=/etc/kubernetes/pki/ca.crt
      - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      - --service-cluster-ip-range=10.96.0.0/12
      - --use-service-account-credentials=true
      image: registry.k8s.io/kube-controller-manager:v1.29.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-controller-manager
      resources:
        requests:
          cpu: 200m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        name: flexvolume-dir
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /etc/kubernetes/controller-manager.conf
        name: kubeconfig
        readOnly: true
      - mountPath: /usr/local/share/ca-certificates
        name: usr-local-share-ca-certificates
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-share-ca-certificates
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        type: DirectoryOrCreate
      name: flexvolume-dir
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /etc/kubernetes/controller-manager.conf
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /usr/local/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-local-share-ca-certificates
    - hostPath:
        path: /usr/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-share-ca-certificates
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:52Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a2f261d3b3442839e532124759ff17108f8f3741f085d298c1876869d7f58c20
      image: registry.k8s.io/kube-controller-manager:v1.29.3
      imageID: registry.k8s.io/kube-controller-manager@sha256:5a7968649f8aee83d5a2d75d6d377ba2680df25b0b97b3be12fa10f15ad67104
      lastState: {}
      name: kube-controller-manager
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2024-04-01T03:46:15Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 139.178.83.85
    podIPs:
    - ip: 139.178.83.85
    qosClass: Burstable
    startTime: "2024-11-06T05:50:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-04-01T03:46:34Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: "7659797656"
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-95kdh
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: e13a2b45-a00d-48fb-b126-6712c3236b8a
    resourceVersion: "401"
    uid: 791e87b1-086d-4f5b-8859-1eba5ab70bb8
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - c3-medium-x86-03-meshery
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/kube-proxy:v1.29.3
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dvv2v
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: c3-medium-x86-03-meshery
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-dvv2v
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T03:46:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T03:46:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T03:46:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T03:46:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T03:46:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7011670082495a1a27e459203782b6ac9648fc11559415f66259cf48289ce4e5
      image: registry.k8s.io/kube-proxy:v1.29.3
      imageID: registry.k8s.io/kube-proxy@sha256:fa87cba052adcb992bd59bd1304115c6f3b3fb370407805ba52af3d9ff3f0863
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-01T03:46:34Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 139.178.83.85
    podIPs:
    - ip: 139.178.83.85
    qosClass: BestEffort
    startTime: "2024-04-01T03:46:34Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 7d383f77e4929bf2ab3c9645efe57bdf
      kubernetes.io/config.mirror: 7d383f77e4929bf2ab3c9645efe57bdf
      kubernetes.io/config.seen: "2024-04-01T03:46:15.019060393Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2024-04-01T03:46:18Z"
    labels:
      component: kube-scheduler
      tier: control-plane
    name: kube-scheduler-c3-medium-x86-03-meshery
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: c3-medium-x86-03-meshery
      uid: 091022f9-bbba-4427-ada7-00e6e0deb886
    resourceVersion: "47870713"
    uid: 7d2ed284-f97e-474a-8de0-fdee8a12ba53
  spec:
    containers:
    - command:
      - kube-scheduler
      - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      - --bind-address=127.0.0.1
      - --kubeconfig=/etc/kubernetes/scheduler.conf
      - --leader-elect=true
      image: registry.k8s.io/kube-scheduler:v1.29.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-scheduler
      resources:
        requests:
          cpu: 100m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes/scheduler.conf
        name: kubeconfig
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/scheduler.conf
        type: FileOrCreate
      name: kubeconfig
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:52Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a69b0f95e0921536fd5c528783311c0ea3bd03d02e25c9d83577446a87b6e3b6
      image: registry.k8s.io/kube-scheduler:v1.29.3
      imageID: registry.k8s.io/kube-scheduler@sha256:6fb91d791db6d62f6b1ac9dbed23fdb597335550d99ff8333d53c4136e889b3a
      lastState: {}
      name: kube-scheduler
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2024-04-01T03:46:15Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 139.178.83.85
    podIPs:
    - ip: 139.178.83.85
    qosClass: Burstable
    startTime: "2024-11-06T05:50:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-11-11T19:35:31Z"
    generateName: metrics-server-84d85ccf8c-
    labels:
      k8s-app: metrics-server
      pod-template-hash: 84d85ccf8c
    name: metrics-server-84d85ccf8c-m4k2g
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metrics-server-84d85ccf8c
      uid: 070e5ba3-7140-44ba-a400-2edd3d11d8df
    resourceVersion: "49338043"
    uid: 8fa37284-370c-4c06-adf0-8fd105e1caef
  spec:
    containers:
    - args:
      - --cert-dir=/tmp
      - --secure-port=4443
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --kubelet-use-node-status-port
      - --metric-resolution=15s
      - --kubelet-insecure-tls
      image: registry.k8s.io/metrics-server/metrics-server:v0.7.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: metrics-server
      ports:
      - containerPort: 4443
        hostPort: 4443
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: https
          scheme: HTTPS
        initialDelaySeconds: 20
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fsmpj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: c3-medium-x86-03-meshery
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: metrics-server
    serviceAccountName: metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp-dir
    - name: kube-api-access-fsmpj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-11T19:35:32Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-11T19:35:31Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-11T19:35:51Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-11T19:35:51Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-11T19:35:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://45bea74e77b3be51e3787ce096a0af31ff2ac2299b0ccd1c466b2c10395b392d
      image: registry.k8s.io/metrics-server/metrics-server:v0.7.2
      imageID: registry.k8s.io/metrics-server/metrics-server@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9
      lastState: {}
      name: metrics-server
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-11T19:35:31Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 139.178.83.85
    podIPs:
    - ip: 139.178.83.85
    qosClass: Burstable
    startTime: "2024-11-11T19:35:31Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-04-01T03:55:12Z"
    generateName: weave-net-
    labels:
      controller-revision-hash: 778d789685
      name: weave-net
      pod-template-generation: "1"
    name: weave-net-x89sr
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: weave-net
      uid: 532bcd57-b9e9-4ce8-882f-82fba72dac48
    resourceVersion: "47870770"
    uid: 755bc88d-da60-4b36-be5f-2570e298dd96
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - c3-medium-x86-03-meshery
    containers:
    - command:
      - /home/weave/launch.sh
      env:
      - name: INIT_CONTAINER
        value: "true"
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: IPALLOC_RANGE
        value: 192.168.0.0/16
      image: weaveworks/weave-kube:latest
      imagePullPolicy: Always
      name: weave
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          path: /status
          port: 6784
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 50m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /weavedb
        name: weavedb
      - mountPath: /host/var/lib/dbus
        name: dbus
        readOnly: true
      - mountPath: /host/etc/machine-id
        name: cni-machine-id
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9px8d
        readOnly: true
    - env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: weaveworks/weave-npc:latest
      imagePullPolicy: Always
      name: weave-npc
      resources:
        requests:
          cpu: 50m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9px8d
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /home/weave/init.sh
      image: weaveworks/weave-kube:latest
      imagePullPolicy: Always
      name: weave-init
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt
        name: cni-bin
      - mountPath: /host/home
        name: cni-bin2
      - mountPath: /host/etc
        name: cni-conf
      - mountPath: /lib/modules
        name: lib-modules
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9px8d
        readOnly: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seLinuxOptions: {}
    serviceAccount: weave-net
    serviceAccountName: weave-net
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/weave
        type: ""
      name: weavedb
    - hostPath:
        path: /opt
        type: ""
      name: cni-bin
    - hostPath:
        path: /home
        type: ""
      name: cni-bin2
    - hostPath:
        path: /etc
        type: ""
      name: cni-conf
    - hostPath:
        path: /etc/machine-id
        type: ""
      name: cni-machine-id
    - hostPath:
        path: /var/lib/dbus
        type: ""
      name: dbus
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: kube-api-access-9px8d
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T03:55:13Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T03:55:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T03:55:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://510ee84f9ae98ac23324f9dda8f39302a8b058c2aa53249c7a4286072ac8d8be
      image: docker.io/weaveworks/weave-kube:latest
      imageID: docker.io/weaveworks/weave-kube@sha256:35827a9c549c095f0e9d1cf8b35d8f27ae2c76e31bc6f7f3c0bc95911d5accea
      lastState: {}
      name: weave
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-01T03:55:15Z"
    - containerID: containerd://23e79a24fd9019df22db785fb2dc6d01ff4f791d30c92085442fb37acf1ea41d
      image: docker.io/weaveworks/weave-npc:latest
      imageID: docker.io/weaveworks/weave-npc@sha256:062832fd25b5e9e16650e618f26bba1409a7b3bf2c3903e1b369d788abc63aef
      lastState: {}
      name: weave-npc
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-01T03:55:16Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    initContainerStatuses:
    - containerID: containerd://3ed3c7c1e4ced12f45ba9b96cb7a9201afcb1ef08a44796b0c19028b1c82d048
      image: docker.io/weaveworks/weave-kube:latest
      imageID: docker.io/weaveworks/weave-kube@sha256:35827a9c549c095f0e9d1cf8b35d8f27ae2c76e31bc6f7f3c0bc95911d5accea
      lastState: {}
      name: weave-init
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://3ed3c7c1e4ced12f45ba9b96cb7a9201afcb1ef08a44796b0c19028b1c82d048
          exitCode: 0
          finishedAt: "2024-04-01T03:55:14Z"
          reason: Completed
          startedAt: "2024-04-01T03:55:13Z"
    phase: Running
    podIP: 139.178.83.85
    podIPs:
    - ip: 139.178.83.85
    qosClass: Burstable
    startTime: "2024-04-01T03:55:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-04-29T12:30:38Z"
    generateName: local-path-provisioner-6d9d9b57c9-
    labels:
      app: local-path-provisioner
      pod-template-hash: 6d9d9b57c9
    name: local-path-provisioner-6d9d9b57c9-wzst7
    namespace: local-path-storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: local-path-provisioner-6d9d9b57c9
      uid: 45ee6149-05a5-4e53-b85b-15bf97984064
    resourceVersion: "5900390"
    uid: eac003fe-128b-4cd3-871c-274e3c045fc7
  spec:
    containers:
    - command:
      - local-path-provisioner
      - --debug
      - start
      - --config
      - /etc/config/config.json
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/local-path-provisioner:v0.0.26
      imagePullPolicy: IfNotPresent
      name: local-path-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9v6cf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: local-path-provisioner-service-account
    serviceAccountName: local-path-provisioner-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: local-path-config
      name: config-volume
    - name: kube-api-access-9v6cf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-04-29T12:30:40Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-04-29T12:30:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-04-29T12:30:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-04-29T12:30:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-04-29T12:30:38Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3f9c0f9c8502a20eec5d84d1d52c71dc4c4621feb12fbd35b0e2c83733e4158f
      image: docker.io/rancher/local-path-provisioner:v0.0.26
      imageID: docker.io/rancher/local-path-provisioner@sha256:aee53cadc62bd023911e7f077877d047c5b3c269f9bba25724d558654f43cea0
      lastState: {}
      name: local-path-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-29T12:30:40Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.18
    podIPs:
    - ip: 192.168.0.18
    qosClass: BestEffort
    startTime: "2024-04-29T12:30:38Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2024-10-28T02:54:14Z"
    creationTimestamp: "2025-01-09T18:48:41Z"
    generateName: meshery-66b556887f-
    labels:
      app: meshery
      pod-template-hash: 66b556887f
    name: meshery-66b556887f-whr6s
    namespace: meshery-extensions
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: meshery-66b556887f
      uid: c0d896f7-650b-4c05-af75-c755d3e2a1ad
    resourceVersion: "62625101"
    uid: 17b633d6-b5e3-4de0-b29f-27d5225c3cdd
  spec:
    containers:
    - env:
      - name: PROVIDER_BASE_URLS
        value: https://cloud.layer5.io
      - name: PROVIDER
        value: Meshery
      - name: PLAYGROUND
        value: "true"
      - name: DISABLE_OPERATOR
        value: "true"
      - name: CAPABILITIES
      image: layer5/meshery:kanvas-v0.8.8
      imagePullPolicy: Always
      name: meshery
      ports:
      - containerPort: 8080
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fl6h4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-fl6h4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T18:49:01Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T18:48:41Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T18:49:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T18:49:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T18:48:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://58255e36cc3bdefb41a22c29e09ab0ed4ddf61edaaa7f14b4018848ba7a97b70
      image: docker.io/layer5/meshery:kanvas-v0.8.8
      imageID: docker.io/layer5/meshery@sha256:96ccda39d0a15c25450920aa1061481a81aa1c67caabb773d3f3b39970646b5a
      lastState: {}
      name: meshery
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-09T18:49:00Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.28
    podIPs:
    - ip: 192.168.0.28
    qosClass: BestEffort
    startTime: "2025-01-09T18:48:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-01-09T22:47:27Z"
    generateName: meshery-6487b554bd-
    labels:
      app.kubernetes.io/instance: meshery
      app.kubernetes.io/name: meshery
      pod-template-hash: 6487b554bd
    name: meshery-6487b554bd-zrtz5
    namespace: meshery
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: meshery-6487b554bd
      uid: 19081267-fb9a-4e34-a4d4-2f1490fa11f7
    resourceVersion: "62656708"
    uid: fd79b222-4c7c-429f-be5f-491a524cf5f3
  spec:
    containers:
    - env:
      - name: ADAPTER_URLS
        value: meshery-istio:10000 meshery-linkerd:10001 meshery-consul:10002 meshery-kuma:10007
          meshery-nginx-sm:10010 meshery-nsm:10004 meshery-app-mesh:10005 meshery-traefik-mesh:10006
          meshery-cilium:10012
      - name: EVENT
        value: mesheryLocal
      - name: KEYS_PATH
        value: ../../server/permissions/keys.csv
      - name: MESHERY_SERVER_CALLBACK_URL
      - name: PROVIDER
        value: Meshery
      - name: PROVIDER_BASE_URLS
        value: https://cloud.layer5.io
      image: layer5/meshery:edge-latest
      imagePullPolicy: Always
      name: meshery
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      resources: {}
      securityContext: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dx5lj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: meshery-server
    serviceAccountName: meshery-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-dx5lj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:49Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://adc39c36d8310e08aa1b625366566e1464bf65064f6f24f1ea3d6d78cc82b8ce
      image: docker.io/layer5/meshery:edge-latest
      imageID: docker.io/layer5/meshery@sha256:97702282aad7f332045f4997022fcb916d328b5da5c52a87e47702ba13455f27
      lastState: {}
      name: meshery
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-09T22:47:48Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.13
    podIPs:
    - ip: 192.168.0.13
    qosClass: BestEffort
    startTime: "2025-01-09T22:47:27Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      meshery/component-type: management-plane
      prometheus.io/path: /metrics
      prometheus.io/port: "7777"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-01-09T22:47:50Z"
    generateName: meshery-broker-
    labels:
      app: meshery
      apps.kubernetes.io/pod-index: "0"
      component: broker
      controller-revision-hash: meshery-broker-56cff9c4f6
      statefulset.kubernetes.io/pod-name: meshery-broker-0
    name: meshery-broker-0
    namespace: meshery
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: meshery-broker
      uid: a59426f9-344b-4ecd-9fc2-24b055dd564a
    resourceVersion: "62656830"
    uid: 89b88eff-96bf-4879-84af-c8923d511b7b
  spec:
    containers:
    - command:
      - nats-server
      - --config
      - /etc/nats-config/nats.conf
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CLUSTER_ADVERTISE
        value: $(POD_NAME).meshery-nats.$(POD_NAMESPACE).svc
      image: nats:2.8.2-alpine3.15
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - nats-server -sl=ldm=/var/run/nats/nats.pid && /bin/sleep 60
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 8222
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: nats
      ports:
      - containerPort: 4222
        name: client
        protocol: TCP
      - containerPort: 6222
        name: cluster
        protocol: TCP
      - containerPort: 7422
        name: leafnodes
        protocol: TCP
      - containerPort: 7522
        name: gateways
        protocol: TCP
      - containerPort: 8222
        name: monitor
        protocol: TCP
      - containerPort: 7777
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 8222
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/nats-config
        name: config-volume
      - mountPath: /var/run/nats
        name: pid
      - mountPath: /etc/nats-config/accounts
        name: resolver-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8htjt
        readOnly: true
    - command:
      - nats-server-config-reloader
      - -pid
      - /var/run/nats/nats.pid
      - -config
      - /etc/nats-config/nats.conf
      image: connecteverything/nats-server-config-reloader:0.6.0
      imagePullPolicy: IfNotPresent
      name: reloader
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/nats-config
        name: config-volume
      - mountPath: /var/run/nats
        name: pid
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8htjt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: meshery-broker-0
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: meshery-operator
    serviceAccountName: meshery-operator
    shareProcessNamespace: true
    subdomain: meshery-nats
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: meshery-nats-config
      name: config-volume
    - emptyDir: {}
      name: pid
    - configMap:
        defaultMode: 420
        name: meshery-nats-accounts
      name: resolver-volume
    - name: kube-api-access-8htjt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:52Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:48:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:48:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://068a8d18158e2f912d8c8e7dc0412c33e089e0183c7588951a8557bd2bf9053c
      image: docker.io/library/nats:2.8.2-alpine3.15
      imageID: docker.io/library/nats@sha256:51ed1dcadcd928cbca6b4b2846491bd14667dca111ac63fdb001eef89cb9192f
      lastState: {}
      name: nats
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-09T22:47:51Z"
    - containerID: containerd://cf7c61c6df96dce206d520addbd7a523b63aaaac0640b0102dcb14a006628711
      image: docker.io/connecteverything/nats-server-config-reloader:0.6.0
      imageID: docker.io/connecteverything/nats-server-config-reloader@sha256:c5e1af9a7667ce15f036a8bb0984bb59472e5340faeb3cc314538885de21da9f
      lastState: {}
      name: reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-09T22:47:51Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.22
    podIPs:
    - ip: 192.168.0.22
    qosClass: BestEffort
    startTime: "2025-01-09T22:47:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      meshery/component-type: management-plane
    creationTimestamp: "2025-01-09T22:47:50Z"
    generateName: meshery-meshsync-59747c798d-
    labels:
      app: meshery
      component: meshsync
      pod-template-hash: 59747c798d
    name: meshery-meshsync-59747c798d-9r49n
    namespace: meshery
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: meshery-meshsync-59747c798d
      uid: 91d30ddd-dfa8-4eac-83b2-18fa5124a4d7
    resourceVersion: "62656783"
    uid: 21f9c913-e30b-45a0-9b6a-331816e8d73f
  spec:
    containers:
    - command:
      - ./meshery-meshsync
      - --broker-url
      - $(BROKER_URL)
      env:
      - name: BROKER_URL
        value: 10.109.58.0:4222
      image: layer5/meshsync:stable-latest
      imagePullPolicy: Always
      name: meshsync
      ports:
      - containerPort: 11000
        hostPort: 11000
        name: client
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rd5pv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: meshery-operator
    serviceAccountName: meshery-operator
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-rd5pv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:53Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6911f3ea2efb16d508decb7cbf5bac886a00b1dac4acbc5ce749ae182d1b4698
      image: docker.io/layer5/meshsync:stable-latest
      imageID: docker.io/layer5/meshsync@sha256:b3c6942fcb8d0e22295258e1274147b892857506302dd6c8569e80e919e515eb
      lastState: {}
      name: meshsync
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-09T22:47:52Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.23
    podIPs:
    - ip: 192.168.0.23
    qosClass: BestEffort
    startTime: "2025-01-09T22:47:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-01-09T22:47:28Z"
    generateName: meshery-operator-7c77d9b57f-
    labels:
      app.kubernetes.io/instance: meshery-operator
      app.kubernetes.io/name: meshery-operator
      pod-template-hash: 7c77d9b57f
    name: meshery-operator-7c77d9b57f-xb8hj
    namespace: meshery
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: meshery-operator-7c77d9b57f
      uid: 227583ff-454b-4fc2-806e-f02b35fb705c
    resourceVersion: "62656720"
    uid: 5bc8c345-f6c5-49f6-92f2-610edfd199d9
  spec:
    containers:
    - args:
      - --metrics-addr=127.0.0.1:8080
      - --enable-leader-election
      command:
      - /manager
      image: layer5/meshery-operator:stable-latest
      imagePullPolicy: Always
      name: manager
      ports:
      - containerPort: 9443
        name: server
        protocol: TCP
      - containerPort: 8080
        name: metrics
        protocol: TCP
      resources: {}
      securityContext: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vpttp
        readOnly: true
    - args:
      - --secure-listen-address=0.0.0.0:8443
      - --upstream=http://127.0.0.1:8080/
      - --logtostderr=false
      - --v=10
      image: gcr.io/kubebuilder/kube-rbac-proxy:v0.16.0
      imagePullPolicy: Always
      name: kube-rbac-proxy
      ports:
      - containerPort: 8443
        name: https
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vpttp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: meshery-operator
    serviceAccountName: meshery-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-vpttp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:50Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-09T22:47:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7dd74cb043d23e96335389c1736b9dfb9773987abc4f1a9c49a7df6df655c30b
      image: gcr.io/kubebuilder/kube-rbac-proxy:v0.16.0
      imageID: gcr.io/kubebuilder/kube-rbac-proxy@sha256:771a9a173e033a3ad8b46f5c00a7036eaa88c8d8d1fbd89217325168998113ea
      lastState: {}
      name: kube-rbac-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-09T22:47:49Z"
    - containerID: containerd://746223ded1e0dc3cbfe2e29ad866458d1579e3cc8d29e32b0d0aefbbb656e481
      image: docker.io/layer5/meshery-operator:stable-latest
      imageID: docker.io/layer5/meshery-operator@sha256:a23e9192b47012afd66d3341436efdf0bbe878ea5e7db2968f8e93b8d75dfd98
      lastState: {}
      name: manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-09T22:47:48Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.21
    podIPs:
    - ip: 192.168.0.21
    qosClass: BestEffort
    startTime: "2025-01-09T22:47:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-04-01T04:00:06Z"
    generateName: metallb-controller-c65987fdb-
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: metallb
      app.kubernetes.io/name: metallb
      pod-template-hash: c65987fdb
    name: metallb-controller-c65987fdb-fqf5r
    namespace: metallb
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metallb-controller-c65987fdb
      uid: 2124dcf4-9632-4321-b99f-660c475eff76
    resourceVersion: "47870702"
    uid: 3793711f-102f-4769-9102-646c8e8ee254
  spec:
    containers:
    - args:
      - --port=7472
      - --log-level=info
      - --tls-min-version=VersionTLS12
      env:
      - name: METALLB_ML_SECRET_NAME
        value: metallb-memberlist
      - name: METALLB_DEPLOYMENT
        value: metallb-controller
      - name: METALLB_BGP_TYPE
        value: frr
      image: quay.io/metallb/controller:v0.14.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: monitoring
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: controller
      ports:
      - containerPort: 7472
        name: monitoring
        protocol: TCP
      - containerPort: 9443
        name: webhook-server
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: monitoring
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/k8s-webhook-server/serving-certs
        name: cert
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cjxs8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: metallb-controller
    serviceAccountName: metallb-controller
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: cert
      secret:
        defaultMode: 420
        secretName: metallb-webhook-cert
    - name: kube-api-access-cjxs8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:00:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:00:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:50:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:00:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7e1ea21d9e1066fcb05f49cdf8a4fe1eabedc26e87ac87a208182ad77c5ea193
      image: quay.io/metallb/controller:v0.14.4
      imageID: quay.io/metallb/controller@sha256:bbef1ee3e7d34e344b318266a18e50cba94aa08c59610fb9a5450a9947c4ade1
      lastState: {}
      name: controller
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-01T04:00:07Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.5
    podIPs:
    - ip: 192.168.0.5
    qosClass: BestEffort
    startTime: "2024-04-01T04:00:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-04-01T04:00:06Z"
    generateName: metallb-speaker-
    labels:
      app.kubernetes.io/component: speaker
      app.kubernetes.io/instance: metallb
      app.kubernetes.io/name: metallb
      controller-revision-hash: 695ddbf6cd
      pod-template-generation: "1"
    name: metallb-speaker-6znzb
    namespace: metallb
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: metallb-speaker
      uid: 8a96e7a5-a2e3-491d-94f9-4a3b7202fa5a
    resourceVersion: "47870917"
    uid: 5c1a65c6-176e-46c9-a329-98ee86faccaf
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - c3-medium-x86-03-meshery
    containers:
    - args:
      - --port=7472
      - --log-level=info
      env:
      - name: METALLB_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: METALLB_HOST
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: METALLB_ML_BIND_ADDR
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: METALLB_ML_LABELS
        value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker
      - name: METALLB_ML_BIND_PORT
        value: "7946"
      - name: METALLB_ML_SECRET_KEY_PATH
        value: /etc/ml_secret_key
      - name: FRR_CONFIG_FILE
        value: /etc/frr_reloader/frr.conf
      - name: FRR_RELOADER_PID_FILE
        value: /etc/frr_reloader/reloader.pid
      - name: METALLB_BGP_TYPE
        value: frr
      image: quay.io/metallb/speaker:v0.14.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: monitoring
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: speaker
      ports:
      - containerPort: 7472
        hostPort: 7472
        name: monitoring
        protocol: TCP
      - containerPort: 7946
        hostPort: 7946
        name: memberlist-tcp
        protocol: TCP
      - containerPort: 7946
        hostPort: 7946
        name: memberlist-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: monitoring
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_RAW
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ml_secret_key
        name: memberlist
      - mountPath: /etc/frr_reloader
        name: reloader
      - mountPath: /etc/metallb
        name: metallb-excludel2
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-npjfz
        readOnly: true
    - command:
      - /bin/sh
      - -c
      - |
        /sbin/tini -- /usr/lib/frr/docker-start &
        attempts=0
        until [[ -f /etc/frr/frr.log || $attempts -eq 60 ]]; do
          sleep 1
          attempts=$(( $attempts + 1 ))
        done
        tail -f /etc/frr/frr.log
      env:
      - name: TINI_SUBREAPER
        value: "true"
      image: quay.io/frrouting/frr:9.0.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: livez
          port: 7473
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: frr
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
          - SYS_ADMIN
          - NET_BIND_SERVICE
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /livez
          port: 7473
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/frr
        name: frr-sockets
      - mountPath: /etc/frr
        name: frr-conf
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-npjfz
        readOnly: true
    - command:
      - /etc/frr_reloader/frr-reloader.sh
      image: quay.io/frrouting/frr:9.0.2
      imagePullPolicy: IfNotPresent
      name: reloader
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/frr
        name: frr-sockets
      - mountPath: /etc/frr
        name: frr-conf
      - mountPath: /etc/frr_reloader
        name: reloader
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-npjfz
        readOnly: true
    - args:
      - --metrics-port=7473
      command:
      - /etc/frr_metrics/frr-metrics
      image: quay.io/frrouting/frr:9.0.2
      imagePullPolicy: IfNotPresent
      name: frr-metrics
      ports:
      - containerPort: 7473
        hostPort: 7473
        name: monitoring
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/frr
        name: frr-sockets
      - mountPath: /etc/frr
        name: frr-conf
      - mountPath: /etc/frr_metrics
        name: metrics
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-npjfz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /bin/sh
      - -c
      - cp -rLf /tmp/frr/* /etc/frr/
      image: quay.io/frrouting/frr:9.0.2
      imagePullPolicy: IfNotPresent
      name: cp-frr-files
      resources: {}
      securityContext:
        runAsGroup: 101
        runAsUser: 100
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/frr
        name: frr-startup
      - mountPath: /etc/frr
        name: frr-conf
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-npjfz
        readOnly: true
    - command:
      - /bin/sh
      - -c
      - cp -f /frr-reloader.sh /etc/frr_reloader/
      image: quay.io/metallb/speaker:v0.14.4
      imagePullPolicy: IfNotPresent
      name: cp-reloader
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/frr_reloader
        name: reloader
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-npjfz
        readOnly: true
    - command:
      - /bin/sh
      - -c
      - cp -f /frr-metrics /etc/frr_metrics/
      image: quay.io/metallb/speaker:v0.14.4
      imagePullPolicy: IfNotPresent
      name: cp-metrics
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/frr_metrics
        name: metrics
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-npjfz
        readOnly: true
    nodeName: c3-medium-x86-03-meshery
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: metallb-speaker
    serviceAccountName: metallb-speaker
    shareProcessNamespace: true
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: memberlist
      secret:
        defaultMode: 420
        secretName: metallb-memberlist
    - configMap:
        defaultMode: 256
        name: metallb-excludel2
      name: metallb-excludel2
    - emptyDir: {}
      name: frr-sockets
    - configMap:
        defaultMode: 420
        name: metallb-frr-startup
      name: frr-startup
    - emptyDir: {}
      name: frr-conf
    - emptyDir: {}
      name: reloader
    - emptyDir: {}
      name: metrics
    - name: kube-api-access-npjfz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:00:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:00:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:51:02Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-06T05:51:02Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-04-01T04:00:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://70d05d4541097f163c549a6ee7b64a7b95739b7a49c7a29026c260b1e4d6c732
      image: quay.io/frrouting/frr:9.0.2
      imageID: quay.io/frrouting/frr@sha256:086acb1278fe86118345f456a1fbfafb80c34d03f7bca9137da0729a1aee5e9c
      lastState: {}
      name: frr
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-01T04:00:10Z"
    - containerID: containerd://171924efd0672ed82f7a5d077c5515f05e052a69ae6c9c6414a3f5fdaf1c1c59
      image: quay.io/frrouting/frr:9.0.2
      imageID: quay.io/frrouting/frr@sha256:086acb1278fe86118345f456a1fbfafb80c34d03f7bca9137da0729a1aee5e9c
      lastState: {}
      name: frr-metrics
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-01T04:00:10Z"
    - containerID: containerd://36d14384bf61ac6e9e75a8b75a0579a7d603373bedccffcb994936d72b7812b6
      image: quay.io/frrouting/frr:9.0.2
      imageID: quay.io/frrouting/frr@sha256:086acb1278fe86118345f456a1fbfafb80c34d03f7bca9137da0729a1aee5e9c
      lastState: {}
      name: reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-01T04:00:10Z"
    - containerID: containerd://a75efee584682034e1059fd66314135488c1a9be21671e209638aa89219f5d7d
      image: quay.io/metallb/speaker:v0.14.4
      imageID: quay.io/metallb/speaker@sha256:37611bde45a206195df4866e58824929c407f7971ffbbbebbd5ab6e4f0f20234
      lastState: {}
      name: speaker
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-04-01T04:00:10Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    initContainerStatuses:
    - containerID: containerd://e392c3492ffc8a6a7b417ad10f4ae362ff6fca76be5560f9ba165a2d028d3ebb
      image: quay.io/frrouting/frr:9.0.2
      imageID: quay.io/frrouting/frr@sha256:086acb1278fe86118345f456a1fbfafb80c34d03f7bca9137da0729a1aee5e9c
      lastState: {}
      name: cp-frr-files
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://e392c3492ffc8a6a7b417ad10f4ae362ff6fca76be5560f9ba165a2d028d3ebb
          exitCode: 0
          finishedAt: "2024-04-01T04:00:07Z"
          reason: Completed
          startedAt: "2024-04-01T04:00:07Z"
    - containerID: containerd://c5a7e370c443b8ebf488b609c9301c15139c4583fb8088f4a0c93768fe596739
      image: quay.io/metallb/speaker:v0.14.4
      imageID: quay.io/metallb/speaker@sha256:37611bde45a206195df4866e58824929c407f7971ffbbbebbd5ab6e4f0f20234
      lastState: {}
      name: cp-reloader
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://c5a7e370c443b8ebf488b609c9301c15139c4583fb8088f4a0c93768fe596739
          exitCode: 0
          finishedAt: "2024-04-01T04:00:08Z"
          reason: Completed
          startedAt: "2024-04-01T04:00:08Z"
    - containerID: containerd://34127b09f3ff01b89a0f79fa41749d4f1b43e2ff2d729e6863e9ed96db869cc0
      image: quay.io/metallb/speaker:v0.14.4
      imageID: quay.io/metallb/speaker@sha256:37611bde45a206195df4866e58824929c407f7971ffbbbebbd5ab6e4f0f20234
      lastState: {}
      name: cp-metrics
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://34127b09f3ff01b89a0f79fa41749d4f1b43e2ff2d729e6863e9ed96db869cc0
          exitCode: 0
          finishedAt: "2024-04-01T04:00:09Z"
          reason: Completed
          startedAt: "2024-04-01T04:00:09Z"
    phase: Running
    podIP: 139.178.83.85
    podIPs:
    - ip: 139.178.83.85
    qosClass: BestEffort
    startTime: "2024-04-01T04:00:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/hydra-config: c52870726676f7ba3aa19fdf34f8fa1f8f266ed0b1750aa23d5aa00c10e68bf3
      checksum/hydra-secrets: e3d519a3a711383641afd27a94d503b8a4683060923bfda6faa97d805718ed36
    creationTimestamp: "2024-11-26T08:56:06Z"
    generateName: layer5-hydra-7854b8559d-
    labels:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: hydra
      app.kubernetes.io/version: v1.11.8
      helm.sh/chart: hydra-0.24.2
      pod-template-hash: 7854b8559d
    name: layer5-hydra-7854b8559d-m654g
    namespace: prod-cloud
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: layer5-hydra-7854b8559d
      uid: e70be5ec-fc63-4d45-8899-e14289ad89b7
    resourceVersion: "53125602"
    uid: 67b3004d-222f-45e7-b339-f275069c8886
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - serve
      - all
      - --dangerous-force-http
      - --config
      - /etc/config/hydra.yaml
      command:
      - hydra
      env:
      - name: URLS_SELF_ISSUER
        value: https://cloud.layer5.io/hydra
      - name: DSN
        valueFrom:
          secretKeyRef:
            key: dsn
            name: layer5-hydra
      - name: SECRETS_SYSTEM
        valueFrom:
          secretKeyRef:
            key: secretsSystem
            name: layer5-hydra
      - name: SECRETS_COOKIE
        valueFrom:
          secretKeyRef:
            key: secretsCookie
            name: layer5-hydra
      image: oryd/hydra:v1.11.8
      imagePullPolicy: IfNotPresent
      lifecycle: {}
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health/alive
          port: http-admin
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: hydra
      ports:
      - containerPort: 4444
        name: http-public
        protocol: TCP
      - containerPort: 4445
        name: http-admin
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /health/ready
          port: http-admin
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 100
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: hydra-config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8rvhh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: layer5-hydra
    serviceAccountName: layer5-hydra
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: layer5-hydra
      name: hydra-config-volume
    - name: kube-api-access-8rvhh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-26T08:56:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-26T08:56:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-26T08:56:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-26T08:56:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-26T08:56:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://32249034b6041a5a45a53c6e7b5d9e2d0729fd79c53c96301c270218ac1d8aa7
      image: docker.io/oryd/hydra:v1.11.8
      imageID: docker.io/oryd/hydra@sha256:6314bcd5b7f536773cbc0033a325c3a980cabe23f21cc47148e23bbe9a824341
      lastState: {}
      name: hydra
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-26T08:56:07Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.19
    podIPs:
    - ip: 192.168.0.19
    qosClass: BestEffort
    startTime: "2024-11-26T08:56:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-11-26T08:56:06Z"
    generateName: layer5-hydra-maester-5c558b9b7b-
    labels:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/name: layer5-hydra-maester
      control-plane: controller-manager
      pod-template-hash: 5c558b9b7b
    name: layer5-hydra-maester-5c558b9b7b-8x6gw
    namespace: prod-cloud
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: layer5-hydra-maester-5c558b9b7b
      uid: e157530d-a1bb-49e9-bf63-35b12f7dfd95
    resourceVersion: "53125521"
    uid: 35cfa1e3-2792-4947-ad88-be09250f81ba
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --metrics-addr=127.0.0.1:8080
      - --hydra-url=http://layer5-hydra-admin
      - --hydra-port=4445
      command:
      - /manager
      image: oryd/hydra-maester:v0.0.25
      imagePullPolicy: IfNotPresent
      name: hydra-maester
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-q9qsz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: layer5-hydra-maester-account
    serviceAccountName: layer5-hydra-maester-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-q9qsz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-26T08:56:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-26T08:56:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-26T08:56:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-26T08:56:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-26T08:56:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9917a61c010861fc2cc390bc21bc51384570f6ff7bd4f7d9aa28cde9a02cea2c
      image: docker.io/oryd/hydra-maester:v0.0.25
      imageID: docker.io/oryd/hydra-maester@sha256:33dd0103b25b4a5fbd169245ed89686b51ce9abb07ce868083ddf808f8dabf86
      lastState: {}
      name: hydra-maester
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-26T08:56:07Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.17
    podIPs:
    - ip: 192.168.0.17
    qosClass: BestEffort
    startTime: "2024-11-26T08:56:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/kratos-config: cb52a43523bc090ee27026eac852b8ec94894047102722d74af5a2706eceec68
      checksum/kratos-secrets: 7dcec3ca1f0bc10d1405fa56357752136e7d08fac3fbfcce2baf56236d695d1f
      checksum/kratos-templates: 06dc42590b5035ee28993ffdf0195d39cca757fd9ebb428f06d4935bf3c03af6
    creationTimestamp: "2025-01-06T20:42:03Z"
    generateName: layer5-kratos-courier-
    labels:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/name: layer5-kratos-courier
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: layer5-kratos-courier-56c9cfd59f
      statefulset.kubernetes.io/pod-name: layer5-kratos-courier-0
    name: layer5-kratos-courier-0
    namespace: prod-cloud
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: layer5-kratos-courier
      uid: 4d7d6dad-6662-4465-8ca1-3cd5d38cbd4d
    resourceVersion: "62021120"
    uid: 40e4b128-3029-40b1-afd7-8ec554543fe4
  spec:
    containers:
    - args:
      - courier
      - watch
      - --config
      - /etc/config/kratos.yaml
      env:
      - name: LOG_FORMAT
        value: json
      - name: LOG_LEVEL
        value: trace
      - name: DSN
        valueFrom:
          secretKeyRef:
            key: dsn
            name: layer5-kratos
      - name: SECRETS_DEFAULT
        valueFrom:
          secretKeyRef:
            key: secretsDefault
            name: layer5-kratos
            optional: true
      - name: SECRETS_COOKIE
        valueFrom:
          secretKeyRef:
            key: secretsCookie
            name: layer5-kratos
            optional: true
      - name: SECRETS_CIPHER
        valueFrom:
          secretKeyRef:
            key: secretsCipher
            name: layer5-kratos
            optional: true
      - name: COURIER_SMTP_CONNECTION_URI
        valueFrom:
          secretKeyRef:
            key: smtpConnectionURI
            name: layer5-kratos
      image: oryd/kratos:v1.0.0
      imagePullPolicy: IfNotPresent
      name: layer5-kratos-courier
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seLinuxOptions:
          level: s0:c123,c456
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: layer5-kratos-config-volume
        readOnly: true
      - mountPath: /conf/courier-templates/recovery/valid
        name: kratos-template-recovery-valid-volume
        readOnly: true
      - mountPath: /conf/courier-templates/verification/valid
        name: kratos-template-verification-valid-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-76pwx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: layer5-kratos-courier-0
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      fsGroupChangePolicy: OnRootMismatch
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: layer5-kratos
    serviceAccountName: layer5-kratos
    subdomain: layer5-kratos-courier
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: layer5-kratos-config
      name: layer5-kratos-config-volume
    - configMap:
        defaultMode: 420
        name: layer5-kratos-template-recovery-valid
      name: kratos-template-recovery-valid-volume
    - configMap:
        defaultMode: 420
        name: layer5-kratos-template-verification-valid
      name: kratos-template-verification-valid-volume
    - name: kube-api-access-76pwx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-06T20:42:04Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-06T20:42:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-06T20:42:04Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-06T20:42:04Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-06T20:42:03Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2c37a77ad6e9cab1fd3c85afe52485a9345007a142526a8c84b5f3c6be27a002
      image: docker.io/oryd/kratos:v1.0.0
      imageID: docker.io/oryd/kratos@sha256:d06fc5845f632ef03461a92df00986ade16e211848d96d646cbe96df5831e015
      lastState: {}
      name: layer5-kratos-courier
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-06T20:42:03Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.26
    podIPs:
    - ip: 192.168.0.26
    qosClass: BestEffort
    startTime: "2025-01-06T20:42:03Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/kratos-config: cb52a43523bc090ee27026eac852b8ec94894047102722d74af5a2706eceec68
      checksum/kratos-secrets: 01c4ccd7b5521b90e3f3f39e7d0df69865c582a07b9121131b474e18f69efd9f
      checksum/kratos-templates: 06dc42590b5035ee28993ffdf0195d39cca757fd9ebb428f06d4935bf3c03af6
    creationTimestamp: "2025-01-06T20:42:02Z"
    generateName: layer5-kratos-d67fc5d44-
    labels:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kratos
      app.kubernetes.io/version: v1.0.0
      helm.sh/chart: kratos-0.39.1
      pod-template-hash: d67fc5d44
    name: layer5-kratos-d67fc5d44-79srp
    namespace: prod-cloud
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: layer5-kratos-d67fc5d44
      uid: d07d7262-39e3-44ec-9380-e4f6116dd7c5
    resourceVersion: "62021178"
    uid: 8c818e05-ec4e-470a-903c-54d1bdbf6629
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - serve
      - all
      - --config
      - /etc/config/kratos.yaml
      command:
      - kratos
      env:
      - name: DSN
        valueFrom:
          secretKeyRef:
            key: dsn
            name: layer5-kratos
      - name: SECRETS_DEFAULT
        valueFrom:
          secretKeyRef:
            key: secretsDefault
            name: layer5-kratos
            optional: true
      - name: SECRETS_COOKIE
        valueFrom:
          secretKeyRef:
            key: secretsCookie
            name: layer5-kratos
            optional: true
      - name: SECRETS_CIPHER
        valueFrom:
          secretKeyRef:
            key: secretsCipher
            name: layer5-kratos
            optional: true
      - name: COURIER_SMTP_CONNECTION_URI
        valueFrom:
          secretKeyRef:
            key: smtpConnectionURI
            name: layer5-kratos
      image: oryd/kratos:v1.0.0
      imagePullPolicy: IfNotPresent
      lifecycle: {}
      livenessProbe:
        failureThreshold: 5
        httpGet:
          httpHeaders:
          - name: Host
            value: 127.0.0.1
          path: /admin/health/alive
          port: 4434
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: kratos
      ports:
      - containerPort: 4434
        name: http-admin
        protocol: TCP
      - containerPort: 4433
        name: http-public
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          httpHeaders:
          - name: Host
            value: 127.0.0.1
          path: /admin/health/ready
          port: 4434
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seLinuxOptions:
          level: s0:c123,c456
        seccompProfile:
          type: RuntimeDefault
      startupProbe:
        failureThreshold: 60
        httpGet:
          httpHeaders:
          - name: Host
            value: 127.0.0.1
          path: /admin/health/ready
          port: 4434
          scheme: HTTP
        periodSeconds: 1
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: kratos-config-volume
        readOnly: true
      - mountPath: /conf/courier-templates/recovery/valid
        name: kratos-template-recovery-valid-volume
        readOnly: true
      - mountPath: /conf/courier-templates/verification/valid
        name: kratos-template-verification-valid-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hzg9p
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      fsGroupChangePolicy: OnRootMismatch
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: layer5-kratos
    serviceAccountName: layer5-kratos
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: layer5-kratos-config
      name: kratos-config-volume
    - configMap:
        defaultMode: 420
        name: layer5-kratos-template-recovery-valid
      name: kratos-template-recovery-valid-volume
    - configMap:
        defaultMode: 420
        name: layer5-kratos-template-verification-valid
      name: kratos-template-verification-valid-volume
    - name: kube-api-access-hzg9p
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-06T20:42:04Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-06T20:42:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-06T20:42:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-06T20:42:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-06T20:42:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ab683034a3b2287ce39b46c0a0592b29025fccfcd34b1484f1fbe7c51fef45f4
      image: docker.io/oryd/kratos:v1.0.0
      imageID: docker.io/oryd/kratos@sha256:d06fc5845f632ef03461a92df00986ade16e211848d96d646cbe96df5831e015
      lastState: {}
      name: kratos
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-06T20:42:03Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.25
    podIPs:
    - ip: 192.168.0.25
    qosClass: BestEffort
    startTime: "2025-01-06T20:42:02Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-01-06T20:42:02Z"
    generateName: meshery-cloud-6d4b8bdb67-
    labels:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/name: layer5-cloud
      pod-template-hash: 6d4b8bdb67
    name: meshery-cloud-6d4b8bdb67-gqxzg
    namespace: prod-cloud
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: meshery-cloud-6d4b8bdb67
      uid: b3be0149-e244-4d3e-acf1-6dc407ee1507
    resourceVersion: "63490500"
    uid: 331e0483-5da7-4277-b352-882c9cdd81cf
  spec:
    containers:
    - envFrom:
      - configMapRef:
          name: meshery-cloud-cm
      - secretRef:
          name: meshery-cloud-secret
      image: layer5/meshery-cloud:production-v0.8.119
      imagePullPolicy: IfNotPresent
      name: layer5-cloud
      ports:
      - containerPort: 9876
        name: http
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2cvnn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: github-app-key
      secret:
        defaultMode: 420
        secretName: meshery-cloud
    - name: kube-api-access-2cvnn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-06T20:42:10Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-06T20:42:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-13T16:37:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-13T16:37:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-06T20:42:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://14f55b0749a347b5279087074ec717b260e1569d69a385b9a62fc21279b63e44
      image: docker.io/layer5/meshery-cloud:production-v0.8.119
      imageID: docker.io/layer5/meshery-cloud@sha256:732e128639bb5ed1585425fc32894fb71ba4d4c983bf36c539f2b865dc5bf071
      lastState:
        terminated:
          containerID: containerd://102ff186adcb72b9a23cb55d69531d21e0ba324a9c2fb208a0bbb0e987026efb
          exitCode: 2
          finishedAt: "2025-01-13T16:37:33Z"
          reason: Error
          startedAt: "2025-01-13T14:32:09Z"
      name: layer5-cloud
      ready: true
      restartCount: 33
      started: true
      state:
        running:
          startedAt: "2025-01-13T16:37:33Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.24
    podIPs:
    - ip: 192.168.0.24
    qosClass: BestEffort
    startTime: "2025-01-06T20:42:02Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/extended-configuration: 34199d0d7cc8de5820d3d988558376cd7170e73b7afaacdb0cb2e6249be37bf2
    creationTimestamp: "2024-12-06T15:05:37Z"
    generateName: postgresql-primary-
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: postgresql-primary-b78474b8b
      helm.sh/chart: postgresql-14.0.1
      statefulset.kubernetes.io/pod-name: postgresql-primary-0
    name: postgresql-primary-0
    namespace: prod-cloud
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: postgresql-primary
      uid: f4b92ce2-2787-431d-ae33-7f0e0a0d85c1
    resourceVersion: "55083283"
    uid: a61e8e2a-2bc7-4913-b322-3f033b7d1321
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: primary
                app.kubernetes.io/instance: postgresql
                app.kubernetes.io/name: postgresql
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "true"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: postgres-password
            name: postgresql
      - name: POSTGRES_DATABASE
        value: postgres
      - name: POSTGRES_REPLICATION_MODE
        value: master
      - name: POSTGRES_REPLICATION_USER
        value: repl_user
      - name: POSTGRES_REPLICATION_PASSWORD
        valueFrom:
          secretKeyRef:
            key: replication-password
            name: postgresql
      - name: POSTGRES_CLUSTER_APP_NAME
        value: my_application
      - name: POSTGRES_INITSCRIPTS_USERNAME
        value: postgres
      - name: POSTGRES_INITSCRIPTS_PASSWORD
        value: postgres
      - name: POSTGRESQL_ENABLE_LDAP
        value: "no"
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "false"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit, pg_cron
      image: docker.io/layer5/postgres-ext:12.19-bitnami
      imagePullPolicy: Always
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - exec pg_isready -U "postgres" -d "dbname=postgres" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: tcp-postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - -e
          - |
            exec pg_isready -U "postgres" -d "dbname=postgres" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /docker-entrypoint-initdb.d/
        name: custom-init-scripts
      - mountPath: /bitnami/postgresql/conf/conf.d/
        name: postgresql-extended-config
      - mountPath: /dev/shm
        name: dshm
      - mountPath: /bitnami/postgresql
        name: data
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: postgresql-primary-0
    initContainers:
    - command:
      - /bin/sh
      - -ec
      - |
        chown 1001:1001 /bitnami/postgresql
        mkdir -p /bitnami/postgresql/data
        chmod 700 /bitnami/postgresql/data
        find /bitnami/postgresql -mindepth 1 -maxdepth 1 -not -name "conf" -not -name ".snapshot" -not -name "lost+found" | \
          xargs -r chown -R 1001:1001
        chmod -R 777 /dev/shm
      image: docker.io/bitnami/os-shell:11-debian-11-r96
      imagePullPolicy: IfNotPresent
      name: init-chmod-data
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/postgresql
        name: data
      - mountPath: /dev/shm
        name: dshm
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: postgresql
    serviceAccountName: postgresql
    subdomain: postgresql-primary-hl
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-postgresql-primary-0
    - configMap:
        defaultMode: 420
        name: postgresql-primary-extended-configuration
      name: postgresql-extended-config
    - configMap:
        defaultMode: 420
        name: postgresql-primary-init-scripts
      name: custom-init-scripts
    - emptyDir:
        medium: Memory
      name: dshm
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:05:38Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:05:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:05:48Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:05:48Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:05:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://137c78fad40a1dc366c62e24e5a236e8c3bb09670981b0a919a0f430540d63d9
      image: docker.io/layer5/postgres-ext:12.19-bitnami
      imageID: docker.io/layer5/postgres-ext@sha256:9c343b1a05ff79af483d4ebb2e60ed6b9c8d65afdb50bee38c5158d5723e7ff6
      lastState: {}
      name: postgresql
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-12-06T15:05:39Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    initContainerStatuses:
    - containerID: containerd://90c8e8c1420c85494f0520ec32fe4ac64501e00bec18edb056c75cf22c28b4fe
      image: docker.io/bitnami/os-shell:11-debian-11-r96
      imageID: docker.io/bitnami/os-shell@sha256:8643af4facffb20e14d135b9e5bdae7f4e604db1e755896f60ea55d05ca09287
      lastState: {}
      name: init-chmod-data
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://90c8e8c1420c85494f0520ec32fe4ac64501e00bec18edb056c75cf22c28b4fe
          exitCode: 0
          finishedAt: "2024-12-06T15:05:38Z"
          reason: Completed
          startedAt: "2024-12-06T15:05:38Z"
    phase: Running
    podIP: 192.168.0.10
    podIPs:
    - ip: 192.168.0.10
    qosClass: Burstable
    startTime: "2024-12-06T15:05:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-12-06T15:05:37Z"
    generateName: postgresql-read-
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: postgresql-read-744ffbf4c8
      helm.sh/chart: postgresql-14.0.1
      statefulset.kubernetes.io/pod-name: postgresql-read-0
    name: postgresql-read-0
    namespace: prod-cloud
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: postgresql-read
      uid: 3b02cea0-3f88-4f39-989f-df10a4f59666
    resourceVersion: "55882232"
    uid: d6fe01e6-89b2-4906-be37-a175b3cc67d7
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: read
                app.kubernetes.io/instance: postgresql
                app.kubernetes.io/name: postgresql
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "true"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: postgres-password
            name: postgresql
      - name: POSTGRES_REPLICATION_MODE
        value: slave
      - name: POSTGRES_REPLICATION_USER
        value: repl_user
      - name: POSTGRES_REPLICATION_PASSWORD
        valueFrom:
          secretKeyRef:
            key: replication-password
            name: postgresql
      - name: POSTGRES_CLUSTER_APP_NAME
        value: my_application
      - name: POSTGRES_MASTER_HOST
        value: postgresql-primary
      - name: POSTGRES_MASTER_PORT_NUMBER
        value: "5432"
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "false"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit, pg_cron
      image: docker.io/layer5/postgres-ext:12.19-bitnami
      imagePullPolicy: Always
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - exec pg_isready -U "postgres" -d "dbname=postgres" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: tcp-postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - -e
          - |
            exec pg_isready -U "postgres" -d "dbname=postgres" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dev/shm
        name: dshm
      - mountPath: /bitnami/postgresql
        name: data
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: postgresql-read-0
    initContainers:
    - command:
      - /bin/sh
      - -ec
      - |
        chown 1001:1001 /bitnami/postgresql
        mkdir -p /bitnami/postgresql/data
        chmod 700 /bitnami/postgresql/data
        find /bitnami/postgresql -mindepth 1 -maxdepth 1 -not -name "conf" -not -name ".snapshot" -not -name "lost+found" | \
          xargs -r chown -R 1001:1001
        chmod -R 777 /dev/shm
      image: docker.io/bitnami/os-shell:11-debian-11-r96
      imagePullPolicy: IfNotPresent
      name: init-chmod-data
      resources:
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/postgresql
        name: data
      - mountPath: /dev/shm
        name: dshm
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: postgresql
    serviceAccountName: postgresql
    subdomain: postgresql-read-hl
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-postgresql-read-0
    - emptyDir:
        medium: Memory
      name: dshm
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:05:38Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:05:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:05:48Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:05:48Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:05:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://02ef509307173d5366a1f9057dd72d8dbd9cbb33958fcb659bb85b5c4f980f42
      image: docker.io/layer5/postgres-ext:12.19-bitnami
      imageID: docker.io/layer5/postgres-ext@sha256:9c343b1a05ff79af483d4ebb2e60ed6b9c8d65afdb50bee38c5158d5723e7ff6
      lastState: {}
      name: postgresql
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-12-06T15:05:38Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    initContainerStatuses:
    - containerID: containerd://4adaf275719266c61a1347304876129877b38a35f94d906c865dfd47e04ac053
      image: docker.io/bitnami/os-shell:11-debian-11-r96
      imageID: docker.io/bitnami/os-shell@sha256:8643af4facffb20e14d135b9e5bdae7f4e604db1e755896f60ea55d05ca09287
      lastState: {}
      name: init-chmod-data
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://4adaf275719266c61a1347304876129877b38a35f94d906c865dfd47e04ac053
          exitCode: 0
          finishedAt: "2024-12-06T15:05:38Z"
          reason: Completed
          startedAt: "2024-12-06T15:05:38Z"
    phase: Running
    podIP: 192.168.0.8
    podIPs:
    - ip: 192.168.0.8
    qosClass: Burstable
    startTime: "2024-12-06T15:05:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/extended-configuration: f87889db0ef9b135537b114ea9efa54bad8a27cb93848a5c70c95f65bca78ea4
    creationTimestamp: "2024-12-06T15:09:23Z"
    generateName: layer5-db-postgresql-primary-
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: layer5-db
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: layer5-db-postgresql-primary-75974964b
      helm.sh/chart: postgresql-14.0.1
      statefulset.kubernetes.io/pod-name: layer5-db-postgresql-primary-0
    name: layer5-db-postgresql-primary-0
    namespace: prod-db
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: layer5-db-postgresql-primary
      uid: cdb9b4a1-d9ed-4b15-b926-baf49fae57e1
    resourceVersion: "55083957"
    uid: e039c52a-9097-436e-a93b-1984de8a3edd
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: primary
                app.kubernetes.io/instance: layer5-db
                app.kubernetes.io/name: postgresql
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "true"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: postgres-password
            name: layer5-db-postgresql
      - name: POSTGRES_DATABASE
        value: postgres
      - name: POSTGRES_REPLICATION_MODE
        value: master
      - name: POSTGRES_REPLICATION_USER
        value: repl_user
      - name: POSTGRES_REPLICATION_PASSWORD
        valueFrom:
          secretKeyRef:
            key: replication-password
            name: layer5-db-postgresql
      - name: POSTGRES_CLUSTER_APP_NAME
        value: my_application
      - name: POSTGRES_INITSCRIPTS_USERNAME
        value: postgres
      - name: POSTGRES_INITSCRIPTS_PASSWORD
        value: postgres
      - name: POSTGRESQL_ENABLE_LDAP
        value: "no"
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "false"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit, pg_cron
      image: docker.io/layer5/postgres-ext:12.19-bitnami
      imagePullPolicy: Always
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - exec pg_isready -U "postgres" -d "dbname=postgres" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: tcp-postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - -e
          - |
            exec pg_isready -U "postgres" -d "dbname=postgres" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /docker-entrypoint-initdb.d/
        name: custom-init-scripts
      - mountPath: /bitnami/postgresql/conf/conf.d/
        name: postgresql-extended-config
      - mountPath: /dev/shm
        name: dshm
      - mountPath: /bitnami/postgresql
        name: data
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: layer5-db-postgresql-primary-0
    initContainers:
    - command:
      - /bin/sh
      - -ec
      - |
        chown 1001:1001 /bitnami/postgresql
        mkdir -p /bitnami/postgresql/data
        chmod 700 /bitnami/postgresql/data
        find /bitnami/postgresql -mindepth 1 -maxdepth 1 -not -name "conf" -not -name ".snapshot" -not -name "lost+found" | \
          xargs -r chown -R 1001:1001
        chmod -R 777 /dev/shm
      image: docker.io/bitnami/os-shell:11-debian-11-r96
      imagePullPolicy: IfNotPresent
      name: init-chmod-data
      resources: {}
      securityContext:
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/postgresql
        name: data
      - mountPath: /dev/shm
        name: dshm
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: layer5-db-postgresql
    serviceAccountName: layer5-db-postgresql
    subdomain: layer5-db-postgresql-primary-hl
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-layer5-db-postgresql-primary-0
    - configMap:
        defaultMode: 420
        name: layer5-db-postgresql-primary-extended-configuration
      name: postgresql-extended-config
    - configMap:
        defaultMode: 420
        name: layer5-db-postgresql-primary-init-scripts
      name: custom-init-scripts
    - emptyDir:
        medium: Memory
      name: dshm
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:09:25Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:09:25Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:09:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:09:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:09:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3c082cc0e93765c56303118af82a314367eaaf4858255f221884cb5d7bb32f99
      image: docker.io/layer5/postgres-ext:12.19-bitnami
      imageID: docker.io/layer5/postgres-ext@sha256:9c343b1a05ff79af483d4ebb2e60ed6b9c8d65afdb50bee38c5158d5723e7ff6
      lastState: {}
      name: postgresql
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-12-06T15:09:25Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    initContainerStatuses:
    - containerID: containerd://6cfb32a96bb7831d8b25ba224f2f6a6321432cda3ce48f39bea6c23e518c40a1
      image: docker.io/bitnami/os-shell:11-debian-11-r96
      imageID: docker.io/bitnami/os-shell@sha256:8643af4facffb20e14d135b9e5bdae7f4e604db1e755896f60ea55d05ca09287
      lastState: {}
      name: init-chmod-data
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://6cfb32a96bb7831d8b25ba224f2f6a6321432cda3ce48f39bea6c23e518c40a1
          exitCode: 0
          finishedAt: "2024-12-06T15:09:24Z"
          reason: Completed
          startedAt: "2024-12-06T15:09:24Z"
    phase: Running
    podIP: 192.168.0.11
    podIPs:
    - ip: 192.168.0.11
    qosClass: Burstable
    startTime: "2024-12-06T15:09:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-12-06T15:08:54Z"
    generateName: layer5-db-postgresql-read-
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: layer5-db
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: layer5-db-postgresql-read-64c46447b6
      helm.sh/chart: postgresql-14.0.1
      statefulset.kubernetes.io/pod-name: layer5-db-postgresql-read-0
    name: layer5-db-postgresql-read-0
    namespace: prod-db
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: layer5-db-postgresql-read
      uid: de31fe25-0563-47eb-bcfa-4576c25bf790
    resourceVersion: "55083848"
    uid: 7e775f33-8083-474d-9cd1-6be73fa14f93
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: read
                app.kubernetes.io/instance: layer5-db
                app.kubernetes.io/name: postgresql
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "true"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: postgres-password
            name: layer5-db-postgresql
      - name: POSTGRES_REPLICATION_MODE
        value: slave
      - name: POSTGRES_REPLICATION_USER
        value: repl_user
      - name: POSTGRES_REPLICATION_PASSWORD
        valueFrom:
          secretKeyRef:
            key: replication-password
            name: layer5-db-postgresql
      - name: POSTGRES_CLUSTER_APP_NAME
        value: my_application
      - name: POSTGRES_MASTER_HOST
        value: layer5-db-postgresql-primary
      - name: POSTGRES_MASTER_PORT_NUMBER
        value: "5432"
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "false"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit, pg_cron
      image: docker.io/layer5/postgres-ext:12.19-bitnami
      imagePullPolicy: Always
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - exec pg_isready -U "postgres" -d "dbname=postgres" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: tcp-postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - -e
          - |
            exec pg_isready -U "postgres" -d "dbname=postgres" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dev/shm
        name: dshm
      - mountPath: /bitnami/postgresql
        name: data
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: layer5-db-postgresql-read-0
    initContainers:
    - command:
      - /bin/sh
      - -ec
      - |
        chown 1001:1001 /bitnami/postgresql
        mkdir -p /bitnami/postgresql/data
        chmod 700 /bitnami/postgresql/data
        find /bitnami/postgresql -mindepth 1 -maxdepth 1 -not -name "conf" -not -name ".snapshot" -not -name "lost+found" | \
          xargs -r chown -R 1001:1001
        chmod -R 777 /dev/shm
      image: docker.io/bitnami/os-shell:11-debian-11-r96
      imagePullPolicy: IfNotPresent
      name: init-chmod-data
      resources:
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/postgresql
        name: data
      - mountPath: /dev/shm
        name: dshm
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: layer5-db-postgresql
    serviceAccountName: layer5-db-postgresql
    subdomain: layer5-db-postgresql-read-hl
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-layer5-db-postgresql-read-0
    - emptyDir:
        medium: Memory
      name: dshm
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:08:55Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:08:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:09:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:09:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-12-06T15:08:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e24bec337e462ea63d07692ffff827f56e11c348fa94f0df8afe95ba4824e5c3
      image: docker.io/layer5/postgres-ext:12.19-bitnami
      imageID: docker.io/layer5/postgres-ext@sha256:9c343b1a05ff79af483d4ebb2e60ed6b9c8d65afdb50bee38c5158d5723e7ff6
      lastState: {}
      name: postgresql
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-12-06T15:08:56Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    initContainerStatuses:
    - containerID: containerd://1c1fdd5df17a8dad99124c6e0221ff7469f861a34ff19d4ce96a45cd25431aab
      image: docker.io/bitnami/os-shell:11-debian-11-r96
      imageID: docker.io/bitnami/os-shell@sha256:8643af4facffb20e14d135b9e5bdae7f4e604db1e755896f60ea55d05ca09287
      lastState: {}
      name: init-chmod-data
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://1c1fdd5df17a8dad99124c6e0221ff7469f861a34ff19d4ce96a45cd25431aab
          exitCode: 0
          finishedAt: "2024-12-06T15:08:55Z"
          reason: Completed
          startedAt: "2024-12-06T15:08:55Z"
    phase: Running
    podIP: 192.168.0.7
    podIPs:
    - ip: 192.168.0.7
    qosClass: Burstable
    startTime: "2024-12-06T15:08:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: alertmanager
    creationTimestamp: "2024-11-22T07:18:55Z"
    generateName: alertmanager-prometheus-kube-prometheus-alertmanager-
    labels:
      alertmanager: prometheus-kube-prometheus-alertmanager
      app.kubernetes.io/instance: prometheus-kube-prometheus-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: 0.27.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: alertmanager-prometheus-kube-prometheus-alertmanager-9d99c5867
      statefulset.kubernetes.io/pod-name: alertmanager-prometheus-kube-prometheus-alertmanager-0
    name: alertmanager-prometheus-kube-prometheus-alertmanager-0
    namespace: prod-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: alertmanager-prometheus-kube-prometheus-alertmanager
      uid: 8b097e2a-3573-4ec6-8395-eeeb03081c1c
    resourceVersion: "52150613"
    uid: 15cd3d9b-a05e-4a83-8977-5a71239e939a
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - alertmanager
              - key: alertmanager
                operator: In
                values:
                - prometheus-kube-prometheus-alertmanager
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --storage.path=/alertmanager
      - --data.retention=120h
      - --cluster.listen-address=
      - --web.listen-address=:9093
      - --web.external-url=http://prometheus-kube-prometheus-alertmanager.prod-monitoring:9093
      - --web.route-prefix=/
      - --cluster.label=prod-monitoring/prometheus-kube-prometheus-alertmanager
      - --cluster.peer=alertmanager-prometheus-kube-prometheus-alertmanager-0.alertmanager-operated:9094
      - --cluster.reconnect-timeout=5m
      - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/prometheus/alertmanager:v0.27.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: alertmanager
      ports:
      - containerPort: 9093
        name: http-web
        protocol: TCP
      - containerPort: 9094
        name: mesh-tcp
        protocol: TCP
      - containerPort: 9094
        name: mesh-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        requests:
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
      - mountPath: /alertmanager
        name: alertmanager-prometheus-kube-prometheus-alertmanager-db
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rf897
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9093/-/reload
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.78.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rf897
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: alertmanager-prometheus-kube-prometheus-alertmanager-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8081
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.78.1
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8081
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rf897
        readOnly: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: prometheus-kube-prometheus-alertmanager
    serviceAccountName: prometheus-kube-prometheus-alertmanager
    subdomain: alertmanager-operated
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: config-volume
      secret:
        defaultMode: 420
        secretName: alertmanager-prometheus-kube-prometheus-alertmanager-generated
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: alertmanager-prometheus-kube-prometheus-alertmanager-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - name: web-config
      secret:
        defaultMode: 420
        secretName: alertmanager-prometheus-kube-prometheus-alertmanager-web-config
    - emptyDir: {}
      name: alertmanager-prometheus-kube-prometheus-alertmanager-db
    - name: kube-api-access-rf897
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:19:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:19:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:19:25Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:19:25Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d150904aa7067bdd3e7328055cfd31443b9012ec32b6c3ad006a3f6bc492a816
      image: quay.io/prometheus/alertmanager:v0.27.0
      imageID: quay.io/prometheus/alertmanager@sha256:e13b6ed5cb929eeaee733479dce55e10eb3bc2e9c4586c705a4e8da41e5eacf5
      lastState: {}
      name: alertmanager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-22T07:19:18Z"
    - containerID: containerd://aff60a041e189758cbe85a6960a9a393db838e5cf2dd72d4b18c0bf75e58d814
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.78.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:e2dc5623bcdd39fae332fff7cc3b77cc478f4720b758663e2af59b423b86e878
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-22T07:19:18Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    initContainerStatuses:
    - containerID: containerd://0d7e7bc6cd331839e7249249c5f7290ef454e296f7c95cbe6d7467d8f74647d2
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.78.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:e2dc5623bcdd39fae332fff7cc3b77cc478f4720b758663e2af59b423b86e878
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://0d7e7bc6cd331839e7249249c5f7290ef454e296f7c95cbe6d7467d8f74647d2
          exitCode: 0
          finishedAt: "2024-11-22T07:19:04Z"
          reason: Completed
          startedAt: "2024-11-22T07:19:04Z"
    phase: Running
    podIP: 192.168.0.41
    podIPs:
    - ip: 192.168.0.41
    qosClass: Burstable
    startTime: "2024-11-22T07:18:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
      checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
      checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
      kubectl.kubernetes.io/default-container: grafana
    creationTimestamp: "2024-11-22T07:18:43Z"
    generateName: prometheus-grafana-69f9ccfd8d-
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: grafana
      pod-template-hash: 69f9ccfd8d
    name: prometheus-grafana-69f9ccfd8d-5npr6
    namespace: prod-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-grafana-69f9ccfd8d
      uid: 8fcff22c-c2ab-4b00-a363-87f7b3794d25
    resourceVersion: "52150508"
    uid: ca1921a3-981a-4589-b986-2c48ee05caca
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_dashboard
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /tmp/dashboards
      - name: RESOURCE
        value: both
      - name: NAMESPACE
        value: ALL
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: prometheus-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: prometheus-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/dashboards/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.28.0
      imagePullPolicy: IfNotPresent
      name: grafana-sc-dashboard
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hblk5
        readOnly: true
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_datasource
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /etc/grafana/provisioning/datasources
      - name: RESOURCE
        value: both
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: prometheus-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: prometheus-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/datasources/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.28.0
      imagePullPolicy: IfNotPresent
      name: grafana-sc-datasources
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hblk5
        readOnly: true
    - env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: GF_SECURITY_ADMIN_USER
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: prometheus-grafana
      - name: GF_SECURITY_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: prometheus-grafana
      - name: GF_PATHS_DATA
        value: /var/lib/grafana/
      - name: GF_PATHS_LOGS
        value: /var/log/grafana
      - name: GF_PATHS_PLUGINS
        value: /var/lib/grafana/plugins
      - name: GF_PATHS_PROVISIONING
        value: /etc/grafana/provisioning
      image: docker.io/grafana/grafana:11.3.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: grafana
      ports:
      - containerPort: 3000
        name: grafana
        protocol: TCP
      - containerPort: 9094
        name: gossip-tcp
        protocol: TCP
      - containerPort: 9094
        name: gossip-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
        subPath: grafana.ini
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
        name: sc-dashboard-provider
        subPath: provider.yaml
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hblk5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 472
      runAsGroup: 472
      runAsNonRoot: true
      runAsUser: 472
    serviceAccount: prometheus-grafana
    serviceAccountName: prometheus-grafana
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: prometheus-grafana
      name: config
    - emptyDir: {}
      name: storage
    - emptyDir: {}
      name: sc-dashboard-volume
    - configMap:
        defaultMode: 420
        name: prometheus-grafana-config-dashboards
      name: sc-dashboard-provider
    - emptyDir: {}
      name: sc-datasources-volume
    - name: kube-api-access-hblk5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:19:01Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:19:04Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:19:04Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://238a9591c9694d19b5d674034fea793f6ca7ad8b98f543f6ac2960ea74c45aa1
      image: docker.io/grafana/grafana:11.3.0
      imageID: docker.io/grafana/grafana@sha256:a0f881232a6fb71a0554a47d0fe2203b6888fe77f4cefb7ea62bed7eb54e13c3
      lastState: {}
      name: grafana
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-22T07:19:00Z"
    - containerID: containerd://33aeecbde4659097474f7f9e20b6a1f8fb0be8b570a2c653dc4512891580623e
      image: quay.io/kiwigrid/k8s-sidecar:1.28.0
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:4166a019eeafd1f0fef4d867dc5f224f18d84ec8681dbb31f3ca258ecf07bcf2
      lastState: {}
      name: grafana-sc-dashboard
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-22T07:18:53Z"
    - containerID: containerd://3cc2834259449d56811f07bc7e8d2c42972982396cc53cfaa3e8fa57cbd57c37
      image: quay.io/kiwigrid/k8s-sidecar:1.28.0
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:4166a019eeafd1f0fef4d867dc5f224f18d84ec8681dbb31f3ca258ecf07bcf2
      lastState: {}
      name: grafana-sc-datasources
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-22T07:18:53Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.35
    podIPs:
    - ip: 192.168.0.35
    qosClass: BestEffort
    startTime: "2024-11-22T07:18:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-11-22T07:18:43Z"
    generateName: prometheus-kube-prometheus-operator-6d87886878-
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 66.2.1
      chart: kube-prometheus-stack-66.2.1
      heritage: Helm
      pod-template-hash: 6d87886878
      release: prometheus
    name: prometheus-kube-prometheus-operator-6d87886878-8njdj
    namespace: prod-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-kube-prometheus-operator-6d87886878
      uid: 4326a486-7993-4e84-886d-84a8f91b8655
    resourceVersion: "52150468"
    uid: 0351a862-bce1-45a9-ba89-2b2448414fb6
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --kubelet-service=kube-system/prometheus-kube-prometheus-kubelet
      - --kubelet-endpoints=true
      - --kubelet-endpointslice=false
      - --localhost=127.0.0.1
      - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.78.1
      - --config-reloader-cpu-request=0
      - --config-reloader-cpu-limit=0
      - --config-reloader-memory-request=0
      - --config-reloader-memory-limit=0
      - --thanos-default-base-image=quay.io/thanos/thanos:v0.36.1
      - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
      - --web.enable-tls=true
      - --web.cert-file=/cert/cert
      - --web.key-file=/cert/key
      - --web.listen-address=:10250
      - --web.tls-min-version=VersionTLS13
      env:
      - name: GOGC
        value: "30"
      image: quay.io/prometheus-operator/prometheus-operator:v0.78.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: kube-prometheus-stack
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /cert
        name: tls-secret
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-stqm2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: prometheus-kube-prometheus-operator
    serviceAccountName: prometheus-kube-prometheus-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: tls-secret
      secret:
        defaultMode: 420
        secretName: prometheus-kube-prometheus-admission
    - name: kube-api-access-stqm2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:56Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a0192a56f3926309cfc42d9f6e24c4cdf9cbb4cc22aba841eb23eb796a1229b6
      image: quay.io/prometheus-operator/prometheus-operator:v0.78.1
      imageID: quay.io/prometheus-operator/prometheus-operator@sha256:29ccb0536e89f03de442fe2e338b9e381d53b9c500f8217bcb45814bcfc01a0a
      lastState: {}
      name: kube-prometheus-stack
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-22T07:18:55Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.38
    podIPs:
    - ip: 192.168.0.38
    qosClass: BestEffort
    startTime: "2024-11-22T07:18:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-11-22T07:18:43Z"
    generateName: prometheus-kube-state-metrics-66f5694654-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.27.0
      pod-template-hash: 66f5694654
      release: prometheus
    name: prometheus-kube-state-metrics-66f5694654-89hv7
    namespace: prod-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-kube-state-metrics-66f5694654
      uid: fd7469ae-917b-462f-ae4f-20b946ce9814
    resourceVersion: "52150407"
    uid: 0a9729d1-33ff-4f9d-bcbe-2b354618be80
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --port=8080
      - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kube-state-metrics
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vb8lz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: prometheus-kube-state-metrics
    serviceAccountName: prometheus-kube-state-metrics
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-vb8lz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:47Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3f1f2ab2e7b0738fc9eb8b30d9a6e68029d54fda34d80a5128b3ed524c8a6799
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
      imageID: registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:37d841299325c23b56e5951176ce8ef317d537447c0f1b2d2437dddbb1f51165
      lastState: {}
      name: kube-state-metrics
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-22T07:18:46Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 192.168.0.31
    podIPs:
    - ip: 192.168.0.31
    qosClass: BestEffort
    startTime: "2024-11-22T07:18:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: prometheus
    creationTimestamp: "2024-11-22T07:18:55Z"
    generateName: prometheus-prometheus-kube-prometheus-prometheus-
    labels:
      app.kubernetes.io/instance: prometheus-kube-prometheus-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/version: 2.55.1
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: prometheus-prometheus-kube-prometheus-prometheus-65959b9854
      operator.prometheus.io/name: prometheus-kube-prometheus-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: prometheus-kube-prometheus-prometheus
      statefulset.kubernetes.io/pod-name: prometheus-prometheus-kube-prometheus-prometheus-0
    name: prometheus-prometheus-kube-prometheus-prometheus-0
    namespace: prod-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-prometheus-kube-prometheus-prometheus
      uid: 6930d01d-1e0b-4e58-ab35-0bb8fb202dde
    resourceVersion: "52150620"
    uid: 2ded6686-10ca-4aff-9b7b-8634f9210b94
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - prometheus
              - key: prometheus
                operator: In
                values:
                - prometheus-kube-prometheus-prometheus
            topologyKey: kubernetes.io/hostname
          weight: 100
    automountServiceAccountToken: true
    containers:
    - args:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      - --web.enable-lifecycle
      - --web.external-url=http://prometheus-kube-prometheus-prometheus.prod-monitoring:9090
      - --web.route-prefix=/
      - --storage.tsdb.retention.time=10d
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.wal-compression
      - --web.config.file=/etc/prometheus/web_config/web-config.yaml
      image: quay.io/prometheus/prometheus:v2.55.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      name: prometheus
      ports:
      - containerPort: 9090
        name: http-web
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 3
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
      - mountPath: /prometheus
        name: prometheus-prometheus-kube-prometheus-prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
        name: prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cv5sr
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9090/-/reload
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.78.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
        name: prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cv5sr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-prometheus-kube-prometheus-prometheus-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8081
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.78.1
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8081
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
        name: prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cv5sr
        readOnly: true
    nodeName: c3-medium-x86-03-meshery
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: prometheus-kube-prometheus-prometheus
    serviceAccountName: prometheus-kube-prometheus-prometheus
    shareProcessNamespace: false
    subdomain: prometheus-operated
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: prometheus-prometheus-kube-prometheus-prometheus
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: prometheus-prometheus-kube-prometheus-prometheus-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - configMap:
        defaultMode: 420
        name: prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
      name: prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
    - name: web-config
      secret:
        defaultMode: 420
        secretName: prometheus-prometheus-kube-prometheus-prometheus-web-config
    - emptyDir: {}
      name: prometheus-prometheus-kube-prometheus-prometheus-db
    - name: kube-api-access-cv5sr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:19:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:19:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:19:26Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:19:26Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://520c75056427a2c8b3cb6542a147e06036536fde95e57bc40cffff314a112cda
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.78.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:e2dc5623bcdd39fae332fff7cc3b77cc478f4720b758663e2af59b423b86e878
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-22T07:19:15Z"
    - containerID: containerd://8b00969c5b01fbdf98e26e2bbd8962d2d42c9269ce73e1aeac1a24d78d580a4c
      image: quay.io/prometheus/prometheus:v2.55.1
      imageID: quay.io/prometheus/prometheus@sha256:2659f4c2ebb718e7695cb9b25ffa7d6be64db013daba13e05c875451cf51b0d3
      lastState: {}
      name: prometheus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-22T07:19:15Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    initContainerStatuses:
    - containerID: containerd://d682211a4b2c5c81d1190acf3df2a2f4e75c1631c2d8a1feed5ad1e4ab2baa80
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.78.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:e2dc5623bcdd39fae332fff7cc3b77cc478f4720b758663e2af59b423b86e878
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://d682211a4b2c5c81d1190acf3df2a2f4e75c1631c2d8a1feed5ad1e4ab2baa80
          exitCode: 0
          finishedAt: "2024-11-22T07:19:05Z"
          reason: Completed
          startedAt: "2024-11-22T07:19:05Z"
    phase: Running
    podIP: 192.168.0.42
    podIPs:
    - ip: 192.168.0.42
    qosClass: BestEffort
    startTime: "2024-11-22T07:18:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2024-11-22T07:18:43Z"
    generateName: prometheus-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      controller-revision-hash: 6bf7f67466
      helm.sh/chart: prometheus-node-exporter-4.42.0
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: prometheus
    name: prometheus-prometheus-node-exporter-nlln8
    namespace: prod-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-prometheus-node-exporter
      uid: d85a33c8-0883-4216-9fab-33511603960a
    resourceVersion: "55882265"
    uid: 0f3ad9dc-ca09-486e-b461-206bc3c2de6b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - c3-medium-x86-03-meshery
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.8.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: c3-medium-x86-03-meshery
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-prometheus-node-exporter
    serviceAccountName: prometheus-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:46Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:46Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:46Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-22T07:18:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://36fbae9bfc96647935f0fd289f203899c9e59c061da5ea7dea56e00c749290f7
      image: quay.io/prometheus/node-exporter:v1.8.2
      imageID: quay.io/prometheus/node-exporter@sha256:4032c6d5bfd752342c3e631c2f1de93ba6b86c41db6b167b9a35372c139e7706
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-22T07:18:45Z"
    hostIP: 139.178.83.85
    hostIPs:
    - ip: 139.178.83.85
    phase: Running
    podIP: 139.178.83.85
    podIPs:
    - ip: 139.178.83.85
    qosClass: BestEffort
    startTime: "2024-11-22T07:18:43Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2024-04-01T04:29:32Z"
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.14.4
      helm.sh/chart: cert-manager-v1.14.4
    name: cert-manager
    namespace: cert-manager
    resourceVersion: "4382"
    uid: 5f0b999c-0dd6-422d-9a60-306a3fcb6277
  spec:
    clusterIP: 10.98.240.13
    clusterIPs:
    - 10.98.240.13
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-prometheus-servicemonitor
      port: 9402
      protocol: TCP
      targetPort: 9402
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2024-04-01T04:29:32Z"
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.14.4
      helm.sh/chart: cert-manager-v1.14.4
    name: cert-manager-webhook
    namespace: cert-manager
    resourceVersion: "4378"
    uid: 1cef4d6f-9a6b-4f75-a52c-177ff56c2c90
  spec:
    clusterIP: 10.101.114.154
    clusterIPs:
    - 10.101.114.154
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"acme-challenge-service","namespace":"default"},"spec":{"ports":[{"port":80,"targetPort":8089}],"selector":{"acme.cert-manager.io/http01-solver":"true"}}}
    creationTimestamp: "2024-09-06T12:22:43Z"
    name: acme-challenge-service
    namespace: default
    resourceVersion: "33178787"
    uid: d0f33b30-8cd6-412f-a5d7-fadb86c4835d
  spec:
    clusterIP: 10.103.254.55
    clusterIPs:
    - 10.103.254.55
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 80
      protocol: TCP
      targetPort: 8089
    selector:
      acme.cert-manager.io/http01-solver: "true"
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-04-01T03:46:18Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "231"
    uid: 9f526579-dcdb-4f46-9d76-f0c51c655c71
  spec:
    clusterIP: 10.96.0.1
    clusterIPs:
    - 10.96.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/instance":"emissary-apiext","app.kubernetes.io/managed-by":"kubectl_apply_-f_aes-apiext.yaml","app.kubernetes.io/name":"emissary-apiext","app.kubernetes.io/part-of":"emissary-apiext"},"name":"emissary-apiext","namespace":"emissary-system"},"spec":{"ports":[{"name":"https","port":443,"targetPort":"https"}],"selector":{"app.kubernetes.io/instance":"emissary-apiext","app.kubernetes.io/name":"emissary-apiext","app.kubernetes.io/part-of":"emissary-apiext"},"type":"ClusterIP"}}
    creationTimestamp: "2024-10-01T15:29:56Z"
    labels:
      app.kubernetes.io/instance: emissary-apiext
      app.kubernetes.io/managed-by: kubectl_apply_-f_aes-apiext.yaml
      app.kubernetes.io/name: emissary-apiext
      app.kubernetes.io/part-of: emissary-apiext
    name: emissary-apiext
    namespace: emissary-system
    resourceVersion: "38668494"
    uid: dc1271b6-0fc3-4dfd-bf8b-63a8d6e99baf
  spec:
    clusterIP: 10.109.239.234
    clusterIPs:
    - 10.109.239.234
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app.kubernetes.io/instance: emissary-apiext
      app.kubernetes.io/name: emissary-apiext
      app.kubernetes.io/part-of: emissary-apiext
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"acme-challenge-service","namespace":"emissary"},"spec":{"ports":[{"port":80,"targetPort":8089}],"selector":{"acme.cert-manager.io/http01-solver":"true"}}}
    creationTimestamp: "2024-04-01T05:49:24Z"
    name: acme-challenge-service
    namespace: emissary
    resourceVersion: "11443"
    uid: 6963fa21-4899-4142-a318-56d897427e4a
  spec:
    clusterIP: 10.96.21.147
    clusterIPs:
    - 10.96.21.147
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 80
      protocol: TCP
      targetPort: 8089
    selector:
      acme.cert-manager.io/http01-solver: "true"
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      auth.istio.io/8089: NONE
    creationTimestamp: "2024-11-27T01:51:52Z"
    generateName: cm-acme-http-solver-
    labels:
      acme.cert-manager.io/http-domain: "2939030417"
      acme.cert-manager.io/http-token: "1249119978"
      acme.cert-manager.io/http01-solver: "true"
    name: cm-acme-http-solver-vctbd
    namespace: emissary
    ownerReferences:
    - apiVersion: acme.cert-manager.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: Challenge
      name: certs-staging-playground-5-1341959213-421573756
      uid: dc39fecd-c983-4f45-9aec-a7803811d7e4
    resourceVersion: "53251980"
    uid: cf0f5189-d571-4c6b-aef4-b68188d8e610
  spec:
    clusterIP: 10.98.45.44
    clusterIPs:
    - 10.98.45.44
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 31718
      port: 8089
      protocol: TCP
      targetPort: 8089
    selector:
      acme.cert-manager.io/http-domain: "2939030417"
      acme.cert-manager.io/http-token: "1249119978"
      acme.cert-manager.io/http01-solver: "true"
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      a8r.io/bugs: https://github.com/datawire/ambassador/issues
      a8r.io/chat: http://a8r.io/Slack
      a8r.io/dependencies: emissary-ingress-redis.emissary
      a8r.io/description: The Ambassador Edge Stack goes beyond traditional API Gateways
        and Ingress Controllers with the advanced edge features needed to support
        developer self-service and full-cycle development.
      a8r.io/documentation: https://www.getambassador.io/docs/edge-stack/latest/
      a8r.io/owner: Ambassador Labs
      a8r.io/repository: github.com/datawire/ambassador
      a8r.io/support: https://www.getambassador.io/about-us/support/
      meta.helm.sh/release-name: emissary-ingress
      meta.helm.sh/release-namespace: emissary
    creationTimestamp: "2024-04-01T04:11:02Z"
    labels:
      app.kubernetes.io/component: ambassador-service
      app.kubernetes.io/instance: emissary-ingress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: emissary-ingress
      app.kubernetes.io/part-of: emissary-ingress
      helm.sh/chart: emissary-ingress-8.9.1
      product: aes
    name: emissary-ingress
    namespace: emissary
    resourceVersion: "48997884"
    uid: ded342e0-c0d8-4c9c-bf57-e0340ea408e1
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.101.59.242
    clusterIPs:
    - 10.101.59.242
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 30643
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: https
      nodePort: 31201
      port: 443
      protocol: TCP
      targetPort: 8443
    selector:
      app.kubernetes.io/instance: emissary-ingress
      app.kubernetes.io/name: emissary-ingress
      profile: main
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      a8r.io/bugs: https://github.com/datawire/ambassador/issues
      a8r.io/chat: http://a8r.io/Slack
      a8r.io/dependencies: None
      a8r.io/description: The Ambassador Edge Stack admin service for internal use
        and health checks.
      a8r.io/documentation: https://www.getambassador.io/docs/edge-stack/latest/
      a8r.io/owner: Ambassador Labs
      a8r.io/repository: github.com/datawire/ambassador
      a8r.io/support: https://www.getambassador.io/about-us/support/
      meta.helm.sh/release-name: emissary-ingress
      meta.helm.sh/release-namespace: emissary
    creationTimestamp: "2024-04-01T04:11:02Z"
    labels:
      app.kubernetes.io/instance: emissary-ingress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: emissary-ingress
      app.kubernetes.io/part-of: emissary-ingress
      helm.sh/chart: emissary-ingress-8.9.1
      product: aes
      service: ambassador-admin
    name: emissary-ingress-admin
    namespace: emissary
    resourceVersion: "2707"
    uid: a5956546-7f70-4359-9e34-0911867c7517
  spec:
    clusterIP: 10.102.121.179
    clusterIPs:
    - 10.102.121.179
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: ambassador-admin
      port: 8877
      protocol: TCP
      targetPort: admin
    - name: ambassador-snapshot
      port: 8005
      protocol: TCP
      targetPort: 8005
    selector:
      app.kubernetes.io/instance: emissary-ingress
      app.kubernetes.io/name: emissary-ingress
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: emissary-ingress
      meta.helm.sh/release-namespace: emissary
    creationTimestamp: "2024-04-01T04:11:02Z"
    labels:
      app.kubernetes.io/instance: emissary-ingress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: emissary-ingress-agent
      app.kubernetes.io/part-of: emissary-ingress
      helm.sh/chart: emissary-ingress-8.9.1
      product: aes
    name: emissary-ingress-agent
    namespace: emissary
    resourceVersion: "2715"
    uid: 6388e780-754f-4616-8861-d29919fd3994
  spec:
    clusterIP: 10.109.102.75
    clusterIPs:
    - 10.109.102.75
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: emissary-ingress
      app.kubernetes.io/name: emissary-ingress-agent
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"ingress-nginx","app.kubernetes.io/name":"ingress-nginx","app.kubernetes.io/part-of":"ingress-nginx","app.kubernetes.io/version":"1.8.1"},"name":"ingress-nginx-controller","namespace":"ingress-nginx"},"spec":{"externalTrafficPolicy":"Local","ipFamilies":["IPv4"],"ipFamilyPolicy":"SingleStack","ports":[{"appProtocol":"http","name":"http","port":80,"protocol":"TCP","targetPort":"http"},{"appProtocol":"https","name":"https","port":443,"protocol":"TCP","targetPort":"https"}],"selector":{"app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"ingress-nginx","app.kubernetes.io/name":"ingress-nginx"},"type":"LoadBalancer"}}
      metallb.universe.tf/ip-allocated-from-pool: pool-one
    creationTimestamp: "2024-11-09T04:07:12Z"
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.8.1
    name: ingress-nginx-controller
    namespace: ingress-nginx
    resourceVersion: "48997886"
    uid: 5aab8242-8bbc-48b3-b683-7da242cd5743
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.104.13.9
    clusterIPs:
    - 10.104.13.9
    externalTrafficPolicy: Local
    healthCheckNodePort: 30748
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: http
      name: http
      nodePort: 30170
      port: 80
      protocol: TCP
      targetPort: http
    - appProtocol: https
      name: https
      nodePort: 30661
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 139.178.83.85
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"ingress-nginx","app.kubernetes.io/name":"ingress-nginx","app.kubernetes.io/part-of":"ingress-nginx","app.kubernetes.io/version":"1.8.1"},"name":"ingress-nginx-controller-admission","namespace":"ingress-nginx"},"spec":{"ports":[{"appProtocol":"https","name":"https-webhook","port":443,"targetPort":"webhook"}],"selector":{"app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"ingress-nginx","app.kubernetes.io/name":"ingress-nginx"},"type":"ClusterIP"}}
    creationTimestamp: "2024-11-09T04:07:12Z"
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.8.1
    name: ingress-nginx-controller-admission
    namespace: ingress-nginx
    resourceVersion: "48624524"
    uid: 82b59115-278f-483f-9dd5-9f5532ba99d2
  spec:
    clusterIP: 10.99.35.229
    clusterIPs:
    - 10.99.35.229
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: https
      name: https-webhook
      port: 443
      protocol: TCP
      targetPort: webhook
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-04-01T03:46:19Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "261"
    uid: 71f457b6-9a39-4e63-9331-a3d3fc8d9a14
  spec:
    clusterIP: 10.96.0.10
    clusterIPs:
    - 10.96.0.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"metrics-server"},"name":"metrics-server","namespace":"kube-system"},"spec":{"ports":[{"name":"https","port":443,"protocol":"TCP","targetPort":"https"}],"selector":{"k8s-app":"metrics-server"}}}
    creationTimestamp: "2024-11-11T19:32:55Z"
    labels:
      k8s-app: metrics-server
    name: metrics-server
    namespace: kube-system
    resourceVersion: "49337442"
    uid: 4884aa7a-4127-4a96-82cf-2926fece0079
  spec:
    clusterIP: 10.104.156.130
    clusterIPs:
    - 10.104.156.130
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      k8s-app: metrics-server
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prod-monitoring
    creationTimestamp: "2024-11-22T07:18:43Z"
    labels:
      app: kube-prometheus-stack-coredns
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 66.2.1
      chart: kube-prometheus-stack-66.2.1
      heritage: Helm
      jobLabel: coredns
      release: prometheus
    name: prometheus-kube-prometheus-coredns
    namespace: kube-system
    resourceVersion: "52150177"
    uid: 6fc13f0f-44d1-4e1c-9163-dddf4ab4632a
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prod-monitoring
    creationTimestamp: "2024-11-22T07:18:43Z"
    labels:
      app: kube-prometheus-stack-kube-controller-manager
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 66.2.1
      chart: kube-prometheus-stack-66.2.1
      heritage: Helm
      jobLabel: kube-controller-manager
      release: prometheus
    name: prometheus-kube-prometheus-kube-controller-manager
    namespace: kube-system
    resourceVersion: "52150176"
    uid: 2de3b7e3-7ee3-4f12-b58c-b28a98dd5cf5
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10257
      protocol: TCP
      targetPort: 10257
    selector:
      component: kube-controller-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prod-monitoring
    creationTimestamp: "2024-11-22T07:18:43Z"
    labels:
      app: kube-prometheus-stack-kube-etcd
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 66.2.1
      chart: kube-prometheus-stack-66.2.1
      heritage: Helm
      jobLabel: kube-etcd
      release: prometheus
    name: prometheus-kube-prometheus-kube-etcd
    namespace: kube-system
    resourceVersion: "52150175"
    uid: ca427296-a213-4ae4-9320-9326aad7193b
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 2381
      protocol: TCP
      targetPort: 2381
    selector:
      component: etcd
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prod-monitoring
    creationTimestamp: "2024-11-22T07:18:43Z"
    labels:
      app: kube-prometheus-stack-kube-proxy
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 66.2.1
      chart: kube-prometheus-stack-66.2.1
      heritage: Helm
      jobLabel: kube-proxy
      release: prometheus
    name: prometheus-kube-prometheus-kube-proxy
    namespace: kube-system
    resourceVersion: "52150179"
    uid: 8b68ba02-81a4-4dfb-80c8-fdb6f200b5ac
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10249
      protocol: TCP
      targetPort: 10249
    selector:
      k8s-app: kube-proxy
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prod-monitoring
    creationTimestamp: "2024-11-22T07:18:43Z"
    labels:
      app: kube-prometheus-stack-kube-scheduler
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 66.2.1
      chart: kube-prometheus-stack-66.2.1
      heritage: Helm
      jobLabel: kube-scheduler
      release: prometheus
    name: prometheus-kube-prometheus-kube-scheduler
    namespace: kube-system
    resourceVersion: "52150178"
    uid: 40cf9ff0-efbf-4548-8aef-861a97aa1876
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10259
      protocol: TCP
      targetPort: 10259
    selector:
      component: kube-scheduler
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-11-22T07:18:55Z"
    labels:
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: kubelet
      k8s-app: kubelet
    name: prometheus-kube-prometheus-kubelet
    namespace: kube-system
    resourceVersion: "52150422"
    uid: 6733d9da-1106-443d-b5dc-2fca4a3d09f5
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    - IPv6
    ipFamilyPolicy: RequireDualStack
    ports:
    - name: https-metrics
      port: 10250
      protocol: TCP
      targetPort: 10250
    - name: http-metrics
      port: 10255
      protocol: TCP
      targetPort: 10255
    - name: cadvisor
      port: 4194
      protocol: TCP
      targetPort: 4194
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"acme-challenge-service","namespace":"meshery-extensions"},"spec":{"ports":[{"port":80,"targetPort":8089}],"selector":{"acme.cert-manager.io/http01-solver":"true"}}}
    creationTimestamp: "2024-09-06T12:29:05Z"
    name: acme-challenge-service
    namespace: meshery-extensions
    resourceVersion: "33179913"
    uid: 68456e7c-40e3-41e7-a376-1ea5fdea0aa7
  spec:
    clusterIP: 10.104.166.96
    clusterIPs:
    - 10.104.166.96
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 80
      protocol: TCP
      targetPort: 8089
    selector:
      acme.cert-manager.io/http01-solver: "true"
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"meshery"},"name":"meshery","namespace":"meshery-extensions"},"spec":{"ports":[{"name":"meshery-kanvas","port":9081,"targetPort":8080}],"selector":{"app":"meshery"}},"status":{"loadBalancer":{}}}
    creationTimestamp: "2024-09-06T12:28:30Z"
    labels:
      app: meshery
    name: meshery
    namespace: meshery-extensions
    resourceVersion: "49343433"
    uid: 3f1722fe-86a8-43e4-bb4f-17eb0516e8f9
  spec:
    clusterIP: 10.103.201.217
    clusterIPs:
    - 10.103.201.217
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: meshery-kanvas
      port: 9081
      protocol: TCP
      targetPort: 8080
    selector:
      app: meshery
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: meshery
      meta.helm.sh/release-namespace: meshery
    creationTimestamp: "2025-01-09T22:47:27Z"
    labels:
      app.kubernetes.io/instance: meshery
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: meshery
      app.kubernetes.io/version: v0.8.8
      helm.sh/chart: meshery-v0.8.8
    name: meshery
    namespace: meshery
    resourceVersion: "62656603"
    uid: 885cd421-a687-4d69-9025-b11edbfd8254
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.104.7.79
    clusterIPs:
    - 10.104.7.79
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 31444
      port: 9081
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: meshery
      app.kubernetes.io/name: meshery
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meshery/component-type: management-plane
    creationTimestamp: "2025-01-09T22:47:50Z"
    labels:
      app: meshery
      component: broker
    name: meshery-broker
    namespace: meshery
    ownerReferences:
    - apiVersion: meshery.layer5.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: Broker
      name: meshery-broker
      uid: 6190604f-69f5-475a-8390-bd5fac7e723d
    resourceVersion: "62656742"
    uid: 991f8a2c-c0eb-4f20-ba3d-d58293dcc33a
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.109.58.0
    clusterIPs:
    - 10.109.58.0
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: client
      nodePort: 31067
      port: 4222
      protocol: TCP
      targetPort: 4222
    - name: cluster
      nodePort: 32467
      port: 6222
      protocol: TCP
      targetPort: 6222
    - name: monitor
      nodePort: 30375
      port: 8222
      protocol: TCP
      targetPort: 8222
    - name: metrics
      nodePort: 30252
      port: 7777
      protocol: TCP
      targetPort: 7777
    - name: leafnodes
      nodePort: 32006
      port: 7422
      protocol: TCP
      targetPort: 7422
    - name: gateways
      nodePort: 32545
      port: 7522
      protocol: TCP
      targetPort: 7522
    selector:
      app: meshery
      component: broker
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: meshery-operator
      meta.helm.sh/release-namespace: meshery
    creationTimestamp: "2025-01-09T22:47:28Z"
    labels:
      app.kubernetes.io/instance: meshery-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: meshery-operator
      app.kubernetes.io/version: v0.8.8
      helm.sh/chart: meshery-operator-v0.8.8
    name: meshery-operator
    namespace: meshery
    resourceVersion: "62656635"
    uid: 120868f6-91c0-4de7-8cdc-55c927ca3578
  spec:
    clusterIP: 10.100.104.30
    clusterIPs:
    - 10.100.104.30
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 10000
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: meshery-operator
      app.kubernetes.io/name: meshery-operator
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: metallb
      meta.helm.sh/release-namespace: metallb
    creationTimestamp: "2024-04-01T04:00:06Z"
    labels:
      app.kubernetes.io/instance: metallb
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: metallb
      app.kubernetes.io/version: v0.14.4
      helm.sh/chart: metallb-0.14.4
    name: metallb-webhook-service
    namespace: metallb
    resourceVersion: "1598"
    uid: f66e01c8-afb8-4e1f-8958-44e9e77df6c2
  spec:
    clusterIP: 10.109.199.141
    clusterIPs:
    - 10.109.199.141
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 443
      protocol: TCP
      targetPort: 9443
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: metallb
      app.kubernetes.io/name: metallb
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: layer5
      meta.helm.sh/release-namespace: prod-cloud
    creationTimestamp: "2024-11-26T08:56:06Z"
    labels:
      app.kubernetes.io/component: admin
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: hydra
      app.kubernetes.io/version: v1.11.8
      helm.sh/chart: hydra-0.24.2
    name: layer5-hydra-admin
    namespace: prod-cloud
    resourceVersion: "53125402"
    uid: 9d851206-cd2f-4f79-bc48-ac0b0f0cba1c
  spec:
    clusterIP: 10.108.76.196
    clusterIPs:
    - 10.108.76.196
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9001
      protocol: TCP
      targetPort: http-admin
    selector:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/name: hydra
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: layer5
      meta.helm.sh/release-namespace: prod-cloud
    creationTimestamp: "2024-11-26T08:56:06Z"
    labels:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: hydra
      app.kubernetes.io/version: v1.11.8
      helm.sh/chart: hydra-0.24.2
    name: layer5-hydra-public
    namespace: prod-cloud
    resourceVersion: "53125420"
    uid: 6849375f-1cdd-4bb7-ad06-aabeedbab133
  spec:
    clusterIP: 10.99.185.166
    clusterIPs:
    - 10.99.185.166
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9000
      protocol: TCP
      targetPort: http-public
    selector:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/name: hydra
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: layer5
      meta.helm.sh/release-namespace: prod-cloud
    creationTimestamp: "2024-11-26T08:56:06Z"
    labels:
      app.kubernetes.io/component: admin
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kratos
      app.kubernetes.io/version: v1.0.0
      helm.sh/chart: kratos-0.39.1
    name: layer5-kratos-admin
    namespace: prod-cloud
    resourceVersion: "53125416"
    uid: 651d6b93-8afc-421a-8e89-c172e237d7c0
  spec:
    clusterIP: 10.96.215.139
    clusterIPs:
    - 10.96.215.139
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9011
      protocol: TCP
      targetPort: http-admin
    selector:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/name: kratos
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: layer5
      meta.helm.sh/release-namespace: prod-cloud
    creationTimestamp: "2024-11-26T08:56:06Z"
    labels:
      app.kubernetes.io/component: courier
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kratos
      app.kubernetes.io/version: v1.0.0
      helm.sh/chart: kratos-0.39.1
    name: layer5-kratos-courier
    namespace: prod-cloud
    resourceVersion: "53125400"
    uid: 10b0d708-763d-4dc1-898d-67bcda802148
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9010
      protocol: TCP
      targetPort: http-public
    selector:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/name: layer5-kratos-courier
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: layer5
      meta.helm.sh/release-namespace: prod-cloud
    creationTimestamp: "2024-11-26T08:56:06Z"
    labels:
      app.kubernetes.io/component: public
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kratos
      app.kubernetes.io/version: v1.0.0
      helm.sh/chart: kratos-0.39.1
    name: layer5-kratos-public
    namespace: prod-cloud
    resourceVersion: "53125404"
    uid: dc1bc251-6bc0-4d14-a49b-62d60ba2b52f
  spec:
    clusterIP: 10.96.104.0
    clusterIPs:
    - 10.96.104.0
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9010
      protocol: TCP
      targetPort: http-public
    selector:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/name: kratos
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: layer5
      meta.helm.sh/release-namespace: prod-cloud
    creationTimestamp: "2024-11-26T08:56:06Z"
    labels:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: layer5-cloud
      app.kubernetes.io/version: 0.7.13
      meshery-cloud.sh/chart: layer5-cloud-0.1.0
    name: meshery-cloud
    namespace: prod-cloud
    resourceVersion: "53125412"
    uid: ff99f577-e772-47df-95ae-ea569325c090
  spec:
    clusterIP: 10.106.185.118
    clusterIPs:
    - 10.106.185.118
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9876
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/name: layer5-cloud
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: postgresql
      meta.helm.sh/release-namespace: prod-cloud
    creationTimestamp: "2024-12-06T15:05:37Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-14.0.1
    name: postgresql-primary
    namespace: prod-cloud
    resourceVersion: "55083202"
    uid: 78efcc08-1711-4276-8a0f-5535b2fa0ff8
  spec:
    clusterIP: 10.110.107.120
    clusterIPs:
    - 10.110.107.120
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: postgresql
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: postgresql
      meta.helm.sh/release-namespace: prod-cloud
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    creationTimestamp: "2024-12-06T15:05:37Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-14.0.1
    name: postgresql-primary-hl
    namespace: prod-cloud
    resourceVersion: "55083195"
    uid: 5d0be036-6bd8-4e51-a577-9ea7900c9019
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: postgresql
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: postgresql
      meta.helm.sh/release-namespace: prod-cloud
    creationTimestamp: "2024-12-06T15:05:37Z"
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-14.0.1
    name: postgresql-read
    namespace: prod-cloud
    resourceVersion: "55083198"
    uid: 28b7b4bc-d766-43a4-a371-d74bbf71cd30
  spec:
    clusterIP: 10.106.172.46
    clusterIPs:
    - 10.106.172.46
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    selector:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: postgresql
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: postgresql
      meta.helm.sh/release-namespace: prod-cloud
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    creationTimestamp: "2024-12-06T15:05:37Z"
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-14.0.1
    name: postgresql-read-hl
    namespace: prod-cloud
    resourceVersion: "55083194"
    uid: 7e095cfd-ec7b-4607-950f-42620f356641
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: postgresql
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: layer5-db
      meta.helm.sh/release-namespace: prod-db
    creationTimestamp: "2024-11-26T07:44:31Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: layer5-db
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-14.0.1
    name: layer5-db-postgresql-primary
    namespace: prod-db
    resourceVersion: "53116354"
    uid: adb0588d-6bcf-49fd-97ac-b3eea93071da
  spec:
    clusterIP: 10.110.25.22
    clusterIPs:
    - 10.110.25.22
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: layer5-db
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: layer5-db
      meta.helm.sh/release-namespace: prod-db
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    creationTimestamp: "2024-11-26T07:44:31Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: layer5-db
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-14.0.1
    name: layer5-db-postgresql-primary-hl
    namespace: prod-db
    resourceVersion: "53116347"
    uid: 26b64c0c-378b-4599-bf87-1f34a84653df
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: layer5-db
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: layer5-db
      meta.helm.sh/release-namespace: prod-db
    creationTimestamp: "2024-11-26T07:44:31Z"
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: layer5-db
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-14.0.1
    name: layer5-db-postgresql-read
    namespace: prod-db
    resourceVersion: "53116356"
    uid: 0f7e67de-324d-4344-8765-ccd84afa29ad
  spec:
    clusterIP: 10.108.183.144
    clusterIPs:
    - 10.108.183.144
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    selector:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: layer5-db
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: layer5-db
      meta.helm.sh/release-namespace: prod-db
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    creationTimestamp: "2024-11-26T07:44:31Z"
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: layer5-db
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-14.0.1
    name: layer5-db-postgresql-read-hl
    namespace: prod-db
    resourceVersion: "53116348"
    uid: bc051d85-4454-459d-9a83-784f8f6de1db
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: layer5-db
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-11-22T07:18:55Z"
    labels:
      managed-by: prometheus-operator
      operated-alertmanager: "true"
    name: alertmanager-operated
    namespace: prod-monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Alertmanager
      name: prometheus-kube-prometheus-alertmanager
      uid: a31ef991-da1f-4de4-8d5c-76e1577fdde9
    resourceVersion: "52150429"
    uid: 9dfbe886-eae8-4708-b450-206156bdab50
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: http-web
    - name: tcp-mesh
      port: 9094
      protocol: TCP
      targetPort: 9094
    - name: udp-mesh
      port: 9094
      protocol: UDP
      targetPort: 9094
    selector:
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prod-monitoring
    creationTimestamp: "2024-11-22T07:18:43Z"
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 11.3.0
      helm.sh/chart: grafana-8.6.0
    name: prometheus-grafana
    namespace: prod-monitoring
    resourceVersion: "52150207"
    uid: b161ce9f-8249-4f5f-b4d4-8005bae69ddd
  spec:
    clusterIP: 10.102.42.53
    clusterIPs:
    - 10.102.42.53
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 80
      protocol: TCP
      targetPort: 3000
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: grafana
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prod-monitoring
    creationTimestamp: "2024-11-22T07:18:43Z"
    labels:
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 66.2.1
      chart: kube-prometheus-stack-66.2.1
      heritage: Helm
      release: prometheus
      self-monitor: "true"
    name: prometheus-kube-prometheus-alertmanager
    namespace: prod-monitoring
    resourceVersion: "52150183"
    uid: 95151c10-fab3-4725-908c-a253888c1c13
  spec:
    clusterIP: 10.97.254.100
    clusterIPs:
    - 10.97.254.100
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: 9093
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      alertmanager: prometheus-kube-prometheus-alertmanager
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prod-monitoring
    creationTimestamp: "2024-11-22T07:18:43Z"
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 66.2.1
      chart: kube-prometheus-stack-66.2.1
      heritage: Helm
      release: prometheus
    name: prometheus-kube-prometheus-operator
    namespace: prod-monitoring
    resourceVersion: "52150211"
    uid: a0178392-900a-4f78-a2a9-d26ca77c9f11
  spec:
    clusterIP: 10.110.178.7
    clusterIPs:
    - 10.110.178.7
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app: kube-prometheus-stack-operator
      release: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prod-monitoring
    creationTimestamp: "2024-11-22T07:18:43Z"
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 66.2.1
      chart: kube-prometheus-stack-66.2.1
      heritage: Helm
      release: prometheus
      self-monitor: "true"
    name: prometheus-kube-prometheus-prometheus
    namespace: prod-monitoring
    resourceVersion: "52150190"
    uid: 927f68ad-5e48-4724-811b-96acb6f87de2
  spec:
    clusterIP: 10.104.43.1
    clusterIPs:
    - 10.104.43.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: 9090
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      app.kubernetes.io/name: prometheus
      operator.prometheus.io/name: prometheus-kube-prometheus-prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prod-monitoring
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-11-22T07:18:43Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.27.0
      release: prometheus
    name: prometheus-kube-state-metrics
    namespace: prod-monitoring
    resourceVersion: "52150202"
    uid: feee3868-4caa-48cc-9075-b2428953f459
  spec:
    clusterIP: 10.110.109.38
    clusterIPs:
    - 10.110.109.38
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: kube-state-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-11-22T07:18:55Z"
    labels:
      managed-by: prometheus-operator
      operated-prometheus: "true"
    name: prometheus-operated
    namespace: prod-monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Prometheus
      name: prometheus-kube-prometheus-prometheus
      uid: 6d51de31-c637-4921-a91c-38bed205da12
    resourceVersion: "52150447"
    uid: 5251b687-54ac-447a-b7d4-eb71f5b72041
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: http-web
    selector:
      app.kubernetes.io/name: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prod-monitoring
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-11-22T07:18:43Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      helm.sh/chart: prometheus-node-exporter-4.42.0
      jobLabel: node-exporter
      release: prometheus
    name: prometheus-prometheus-node-exporter
    namespace: prod-monitoring
    resourceVersion: "52150181"
    uid: c36a7b22-c938-4fc4-865f-3aaa22082c78
  spec:
    clusterIP: 10.104.111.238
    clusterIPs:
    - 10.104.111.238
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9100
      protocol: TCP
      targetPort: 9100
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus-node-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2024-04-01T04:29:32Z"
    generation: 1
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.14.4
      helm.sh/chart: cert-manager-v1.14.4
    name: cert-manager
    namespace: cert-manager
    resourceVersion: "4477"
    uid: 158a2254-6986-41ee-8027-13abba632345
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.14.4
          helm.sh/chart: cert-manager-v1.14.4
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.14.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-04-01T04:29:34Z"
      lastUpdateTime: "2024-04-01T04:29:34Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-04-01T04:29:32Z"
      lastUpdateTime: "2024-04-01T04:29:34Z"
      message: ReplicaSet "cert-manager-6dc66985d4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2024-04-01T04:29:32Z"
    generation: 1
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.14.4
      helm.sh/chart: cert-manager-v1.14.4
    name: cert-manager-cainjector
    namespace: cert-manager
    resourceVersion: "4479"
    uid: 0764b7fb-0cab-42d5-8422-bd2581cd932e
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.14.4
          helm.sh/chart: cert-manager-v1.14.4
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-04-01T04:29:34Z"
      lastUpdateTime: "2024-04-01T04:29:34Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-04-01T04:29:32Z"
      lastUpdateTime: "2024-04-01T04:29:34Z"
      message: ReplicaSet "cert-manager-cainjector-c7d4dbdd9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2024-04-01T04:29:32Z"
    generation: 1
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.14.4
      helm.sh/chart: cert-manager-v1.14.4
    name: cert-manager-webhook
    namespace: cert-manager
    resourceVersion: "47870869"
    uid: e890003b-5e6a-4de0-86cc-091b8e15e166
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.14.4
          helm.sh/chart: cert-manager-v1.14.4
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.14.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-04-01T04:29:32Z"
      lastUpdateTime: "2024-04-01T04:29:38Z"
      message: ReplicaSet "cert-manager-webhook-847d7676c9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-11-06T05:50:58Z"
      lastUpdateTime: "2024-11-06T05:50:58Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app.kubernetes.io/instance":"emissary-apiext","app.kubernetes.io/managed-by":"kubectl_apply_-f_aes-apiext.yaml","app.kubernetes.io/name":"emissary-apiext","app.kubernetes.io/part-of":"emissary-apiext"},"name":"emissary-apiext","namespace":"emissary-system"},"spec":{"replicas":3,"selector":{"matchLabels":{"app.kubernetes.io/instance":"emissary-apiext","app.kubernetes.io/name":"emissary-apiext","app.kubernetes.io/part-of":"emissary-apiext"}},"template":{"metadata":{"labels":{"app.kubernetes.io/instance":"emissary-apiext","app.kubernetes.io/managed-by":"kubectl_apply_-f_aes-apiext.yaml","app.kubernetes.io/name":"emissary-apiext","app.kubernetes.io/part-of":"emissary-apiext"}},"spec":{"containers":[{"args":["--crd-label-selector","app.kubernetes.io/part-of=emissary-apiext"],"command":["apiext","emissary-apiext"],"image":"docker.io/datawire/aes:3.11.1","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/probes/live","port":8080,"scheme":"HTTP"},"periodSeconds":3},"name":"emissary-apiext","ports":[{"containerPort":8080,"name":"http"},{"containerPort":8443,"name":"https"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/probes/ready","port":8080,"scheme":"HTTP"},"periodSeconds":3},"startupProbe":{"failureThreshold":10,"httpGet":{"path":"/probes/live","port":8080},"periodSeconds":3}}],"serviceAccountName":"emissary-apiext"}}}}
    creationTimestamp: "2024-10-01T15:29:56Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: emissary-apiext
      app.kubernetes.io/managed-by: kubectl_apply_-f_aes-apiext.yaml
      app.kubernetes.io/name: emissary-apiext
      app.kubernetes.io/part-of: emissary-apiext
    name: emissary-apiext
    namespace: emissary-system
    resourceVersion: "52655137"
    uid: 8840190c-3b83-41f6-bc21-d37569541e2a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: emissary-apiext
        app.kubernetes.io/name: emissary-apiext
        app.kubernetes.io/part-of: emissary-apiext
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: emissary-apiext
          app.kubernetes.io/managed-by: kubectl_apply_-f_aes-apiext.yaml
          app.kubernetes.io/name: emissary-apiext
          app.kubernetes.io/part-of: emissary-apiext
      spec:
        containers:
        - args:
          - --crd-label-selector
          - app.kubernetes.io/part-of=emissary-apiext
          command:
          - apiext
          - emissary-apiext
          image: docker.io/datawire/aes:3.11.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /probes/live
              port: 8080
              scheme: HTTP
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 1
          name: emissary-apiext
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /probes/ready
              port: 8080
              scheme: HTTP
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          startupProbe:
            failureThreshold: 10
            httpGet:
              path: /probes/live
              port: 8080
              scheme: HTTP
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: emissary-apiext
        serviceAccountName: emissary-apiext
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-10-01T15:29:56Z"
      lastUpdateTime: "2024-10-01T15:30:00Z"
      message: ReplicaSet "emissary-apiext-7d468cdc87" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-11-06T05:50:57Z"
      lastUpdateTime: "2024-11-06T05:50:57Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: emissary-ingress
      meta.helm.sh/release-namespace: emissary
    creationTimestamp: "2024-04-01T04:11:02Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: emissary-ingress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: emissary-ingress
      app.kubernetes.io/part-of: emissary-ingress
      helm.sh/chart: emissary-ingress-8.9.1
      product: aes
    name: emissary-ingress
    namespace: emissary
    resourceVersion: "52655064"
    uid: 94e6c3db-74ec-420b-808a-32f5e47c3f12
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: emissary-ingress
        app.kubernetes.io/name: emissary-ingress
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: emissary-ingress
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: emissary-ingress
          app.kubernetes.io/part-of: emissary-ingress
          helm.sh/chart: emissary-ingress-8.9.1
          product: aes
          profile: main
      spec:
        containers:
        - env:
          - name: AMBASSADOR_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: AGENT_CONFIG_RESOURCE_NAME
            value: emissary-ingress-agent-cloud-token
          image: docker.io/emissaryingress/emissary:3.9.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ambassador/v0/check_alive
              port: admin
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 1
          name: emissary-ingress
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8443
            name: https
            protocol: TCP
          - containerPort: 8877
            name: admin
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ambassador/v0/check_ready
              port: admin
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 600Mi
            requests:
              cpu: 200m
              memory: 300Mi
          securityContext:
            allowPrivilegeEscalation: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/ambassador-pod-info
            name: ambassador-pod-info
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - |
            deployment_name="emissary-apiext"
            deployment_namespace="emissary-system"
            while true; do
              echo "checking if deployment/$deployment_name in namespace: $deployment_namespace exists."
              if kubectl get deployment "$deployment_name" -n $deployment_namespace > /dev/null 2>&1; then
                echo "$deployment_name.$deployment_namespace exists."
                echo "checking if $deployment_name.$deployment_namespace is fully available..."
                kubectl wait --for=condition=available deployment/"$deployment_name" -n $deployment_namespace --timeout=5m
                if [ $? -eq 0 ]; then
                  echo "$deployment_name.$deployment_namespace is available"
                  while true; do
                  desired_replicas=$(kubectl get deployment $deployment_name -n $deployment_namespace -o jsonpath='{.spec.replicas}')
                  current_replicas=$(kubectl get deployment $deployment_name -n $deployment_namespace -o jsonpath='{.status.replicas}')
                  if [[ $current_replicas != $desired_replicas ]]; then
                    echo "$deployment_name.$deployment_namespace is in the process of restarting. Have: $current_replicas, want $desired_replicas"
                    sleep 3
                  else
                    echo "$deployment_name.$deployment_namespace is fully ready and not currently restarting.  Have: $current_replicas, want $desired_replicas"
                    break
                  fi
                  done
                  break
                else
                  echo "$deployment_name.$deployment_namespace did not become available within the timeout"
                fi
              else
                echo "$deployment_name.$deployment_namespace does not exist yet. Waiting..."
                sleep 3
              fi
            done
          command:
          - /bin/sh
          - -c
          image: istio/kubectl:1.5.10
          imagePullPolicy: IfNotPresent
          name: wait-for-apiext
          resources: {}
          securityContext:
            runAsUser: 8888
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 8888
        serviceAccount: emissary-ingress
        serviceAccountName: emissary-ingress
        terminationGracePeriodSeconds: 30
        volumes:
        - downwardAPI:
            defaultMode: 420
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.labels
              path: labels
          name: ambassador-pod-info
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-04-01T04:11:02Z"
      lastUpdateTime: "2024-04-01T04:11:36Z"
      message: ReplicaSet "emissary-ingress-5fccb6758d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-11-06T05:50:55Z"
      lastUpdateTime: "2024-11-06T05:50:55Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: emissary-ingress
      meta.helm.sh/release-namespace: emissary
    creationTimestamp: "2024-04-01T04:11:02Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: emissary-ingress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: emissary-ingress-agent
      app.kubernetes.io/part-of: emissary-ingress
      helm.sh/chart: emissary-ingress-8.9.1
      product: aes
    name: emissary-ingress-agent
    namespace: emissary
    resourceVersion: "2778"
    uid: 2a35f1db-e892-4862-9763-42960202f39a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: emissary-ingress
        app.kubernetes.io/name: emissary-ingress-agent
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: emissary-ingress
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: emissary-ingress-agent
          app.kubernetes.io/part-of: emissary-ingress
          helm.sh/chart: emissary-ingress-8.9.1
          product: aes
      spec:
        containers:
        - env:
          - name: AGENT_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: AGENT_CONFIG_RESOURCE_NAME
            value: emissary-ingress-agent-cloud-token
          - name: RPC_CONNECTION_ADDRESS
            value: https://app.getambassador.io/
          - name: AES_SNAPSHOT_URL
            value: http://emissary-ingress-admin.emissary:8005/snapshot-external
          - name: AES_REPORT_DIAGNOSTICS_TO_CLOUD
            value: "true"
          - name: AES_DIAGNOSTICS_URL
            value: http://emissary-ingress-admin.emissary:8877/ambassador/v0/diag/?json=true
          image: docker.io/ambassador/ambassador-agent:1.0.14
          imagePullPolicy: IfNotPresent
          name: agent
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: emissary-ingress-agent
        serviceAccountName: emissary-ingress-agent
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-04-01T04:11:03Z"
      lastUpdateTime: "2024-04-01T04:11:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-04-01T04:11:02Z"
      lastUpdateTime: "2024-04-01T04:11:03Z"
      message: ReplicaSet "emissary-ingress-agent-7f6bb847d8" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"ingress-nginx","app.kubernetes.io/name":"ingress-nginx","app.kubernetes.io/part-of":"ingress-nginx","app.kubernetes.io/version":"1.8.1"},"name":"ingress-nginx-controller","namespace":"ingress-nginx"},"spec":{"minReadySeconds":0,"revisionHistoryLimit":10,"selector":{"matchLabels":{"app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"ingress-nginx","app.kubernetes.io/name":"ingress-nginx"}},"template":{"metadata":{"labels":{"app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"ingress-nginx","app.kubernetes.io/name":"ingress-nginx","app.kubernetes.io/part-of":"ingress-nginx","app.kubernetes.io/version":"1.8.1"}},"spec":{"containers":[{"args":["/nginx-ingress-controller","--publish-service=$(POD_NAMESPACE)/ingress-nginx-controller","--election-id=ingress-nginx-leader","--controller-class=k8s.io/ingress-nginx","--ingress-class=nginx","--configmap=$(POD_NAMESPACE)/ingress-nginx-controller","--validating-webhook=:8443","--validating-webhook-certificate=/usr/local/certificates/cert","--validating-webhook-key=/usr/local/certificates/key"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"LD_PRELOAD","value":"/usr/local/lib/libmimalloc.so"}],"image":"registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd","imagePullPolicy":"IfNotPresent","lifecycle":{"preStop":{"exec":{"command":["/wait-shutdown"]}}},"livenessProbe":{"failureThreshold":5,"httpGet":{"path":"/healthz","port":10254,"scheme":"HTTP"},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":1},"name":"controller","ports":[{"containerPort":80,"name":"http","protocol":"TCP"},{"containerPort":443,"name":"https","protocol":"TCP"},{"containerPort":8443,"name":"webhook","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/healthz","port":10254,"scheme":"HTTP"},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":1},"resources":{"requests":{"cpu":"100m","memory":"90Mi"}},"securityContext":{"allowPrivilegeEscalation":true,"capabilities":{"add":["NET_BIND_SERVICE"],"drop":["ALL"]},"runAsUser":101},"volumeMounts":[{"mountPath":"/usr/local/certificates/","name":"webhook-cert","readOnly":true}]}],"dnsPolicy":"ClusterFirst","nodeSelector":{"kubernetes.io/os":"linux"},"serviceAccountName":"ingress-nginx","terminationGracePeriodSeconds":300,"volumes":[{"name":"webhook-cert","secret":{"secretName":"ingress-nginx-admission"}}]}}}}
    creationTimestamp: "2024-11-09T04:07:12Z"
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.8.1
    name: ingress-nginx-controller
    namespace: ingress-nginx
    resourceVersion: "48624682"
    uid: 9da05cfb-0170-4aa0-8883-3cd46714c143
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/part-of: ingress-nginx
          app.kubernetes.io/version: 1.8.1
      spec:
        containers:
        - args:
          - /nginx-ingress-controller
          - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
          - --election-id=ingress-nginx-leader
          - --controller-class=k8s.io/ingress-nginx
          - --ingress-class=nginx
          - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
          - --validating-webhook=:8443
          - --validating-webhook-certificate=/usr/local/certificates/cert
          - --validating-webhook-key=/usr/local/certificates/key
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_PRELOAD
            value: /usr/local/lib/libmimalloc.so
          image: registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /wait-shutdown
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          - containerPort: 8443
            name: webhook
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 90Mi
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            runAsUser: 101
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/local/certificates/
            name: webhook-cert
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ingress-nginx
        serviceAccountName: ingress-nginx
        terminationGracePeriodSeconds: 300
        volumes:
        - name: webhook-cert
          secret:
            defaultMode: 420
            secretName: ingress-nginx-admission
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-11-09T04:07:36Z"
      lastUpdateTime: "2024-11-09T04:07:36Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-11-09T04:07:12Z"
      lastUpdateTime: "2024-11-09T04:07:36Z"
      message: ReplicaSet "ingress-nginx-controller-568fb54f96" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-04-01T03:46:19Z"
    generation: 1
    labels:
      k8s-app: kube-dns
    name: coredns
    namespace: kube-system
    resourceVersion: "47871076"
    uid: 26cf8969-0717-4bb8-8e91-0bc7da10ae63
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: registry.k8s.io/coredns/coredns:v1.11.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2024-04-01T03:46:34Z"
      lastUpdateTime: "2024-04-01T03:55:26Z"
      message: ReplicaSet "coredns-76f75df574" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-11-06T05:51:17Z"
      lastUpdateTime: "2024-11-06T05:51:17Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"metrics-server"},"name":"metrics-server","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"metrics-server"}},"strategy":{"rollingUpdate":{"maxUnavailable":0}},"template":{"metadata":{"labels":{"k8s-app":"metrics-server"}},"spec":{"containers":[{"args":["--cert-dir=/tmp","--secure-port=10250","--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname","--kubelet-use-node-status-port","--metric-resolution=15s"],"image":"registry.k8s.io/metrics-server/metrics-server:v0.7.2","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/livez","port":"https","scheme":"HTTPS"},"periodSeconds":10},"name":"metrics-server","ports":[{"containerPort":10250,"name":"https","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/readyz","port":"https","scheme":"HTTPS"},"initialDelaySeconds":20,"periodSeconds":10},"resources":{"requests":{"cpu":"100m","memory":"200Mi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]},"readOnlyRootFilesystem":true,"runAsNonRoot":true,"runAsUser":1000,"seccompProfile":{"type":"RuntimeDefault"}},"volumeMounts":[{"mountPath":"/tmp","name":"tmp-dir"}]}],"nodeSelector":{"kubernetes.io/os":"linux"},"priorityClassName":"system-cluster-critical","serviceAccountName":"metrics-server","volumes":[{"emptyDir":{},"name":"tmp-dir"}]}}}}
    creationTimestamp: "2024-11-11T19:32:55Z"
    generation: 2
    labels:
      k8s-app: metrics-server
    name: metrics-server
    namespace: kube-system
    resourceVersion: "49338060"
    uid: 93803f1a-3023-42a9-b138-fb8650d36191
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: metrics-server
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=4443
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --kubelet-insecure-tls
          image: registry.k8s.io/metrics-server/metrics-server:v0.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 4443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-11-11T19:35:51Z"
      lastUpdateTime: "2024-11-11T19:35:51Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-11-11T19:32:56Z"
      lastUpdateTime: "2024-11-11T19:35:51Z"
      message: ReplicaSet "metrics-server-84d85ccf8c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"local-path-provisioner","namespace":"local-path-storage"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"local-path-provisioner"}},"template":{"metadata":{"labels":{"app":"local-path-provisioner"}},"spec":{"containers":[{"command":["local-path-provisioner","--debug","start","--config","/etc/config/config.json"],"env":[{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"rancher/local-path-provisioner:v0.0.26","imagePullPolicy":"IfNotPresent","name":"local-path-provisioner","volumeMounts":[{"mountPath":"/etc/config/","name":"config-volume"}]}],"serviceAccountName":"local-path-provisioner-service-account","volumes":[{"configMap":{"name":"local-path-config"},"name":"config-volume"}]}}}}
    creationTimestamp: "2024-04-29T12:30:38Z"
    generation: 1
    name: local-path-provisioner
    namespace: local-path-storage
    resourceVersion: "5900392"
    uid: 651d0bd1-ed44-4cf3-9b8e-ea02e09eab99
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: local-path-provisioner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
      spec:
        containers:
        - command:
          - local-path-provisioner
          - --debug
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.26
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-04-29T12:30:40Z"
      lastUpdateTime: "2024-04-29T12:30:40Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-04-29T12:30:38Z"
      lastUpdateTime: "2024-04-29T12:30:40Z"
      message: ReplicaSet "local-path-provisioner-6d9d9b57c9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "102"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"meshery"},"name":"meshery","namespace":"meshery-extensions"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"meshery"}},"strategy":{},"template":{"metadata":{"labels":{"app":"meshery"}},"spec":{"containers":[{"env":[{"name":"PROVIDER_BASE_URLS","value":"https://cloud.layer5.io"},{"name":"PROVIDER","value":"Meshery"},{"name":"PLAYGROUND","value":"true"},{"name":"DISABLE_OPERATOR","value":"true"},{"name":"CAPABILITIES","value":""}],"image":"layer5/meshery:kanvas-latest","imagePullPolicy":"Always","name":"meshery","ports":[{"containerPort":8080}],"resources":{}}],"restartPolicy":"Always"}}}}
    creationTimestamp: "2024-09-16T11:58:56Z"
    generation: 102
    labels:
      app: meshery
    name: meshery
    namespace: meshery-extensions
    resourceVersion: "62625116"
    uid: 261aff0b-7c97-45be-af6c-9e5d0382c273
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: meshery
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2024-10-28T02:54:14Z"
        creationTimestamp: null
        labels:
          app: meshery
      spec:
        containers:
        - env:
          - name: PROVIDER_BASE_URLS
            value: https://cloud.layer5.io
          - name: PROVIDER
            value: Meshery
          - name: PLAYGROUND
            value: "true"
          - name: DISABLE_OPERATOR
            value: "true"
          - name: CAPABILITIES
          image: layer5/meshery:kanvas-v0.8.8
          imagePullPolicy: Always
          name: meshery
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-11-26T16:47:20Z"
      lastUpdateTime: "2024-11-26T16:47:20Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-10-18T16:57:13Z"
      lastUpdateTime: "2025-01-09T18:49:01Z"
      message: ReplicaSet "meshery-66b556887f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 102
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: meshery
      meta.helm.sh/release-namespace: meshery
    creationTimestamp: "2025-01-09T22:47:27Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: meshery
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: meshery
      app.kubernetes.io/version: v0.8.8
      helm.sh/chart: meshery-v0.8.8
    name: meshery
    namespace: meshery
    resourceVersion: "62656712"
    uid: 6e4cd60e-729a-47b9-ac4c-265128177cd9
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: meshery
        app.kubernetes.io/name: meshery
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: meshery
          app.kubernetes.io/name: meshery
      spec:
        containers:
        - env:
          - name: ADAPTER_URLS
            value: meshery-istio:10000 meshery-linkerd:10001 meshery-consul:10002
              meshery-kuma:10007 meshery-nginx-sm:10010 meshery-nsm:10004 meshery-app-mesh:10005
              meshery-traefik-mesh:10006 meshery-cilium:10012
          - name: EVENT
            value: mesheryLocal
          - name: KEYS_PATH
            value: ../../server/permissions/keys.csv
          - name: MESHERY_SERVER_CALLBACK_URL
          - name: PROVIDER
            value: Meshery
          - name: PROVIDER_BASE_URLS
            value: https://cloud.layer5.io
          image: layer5/meshery:edge-latest
          imagePullPolicy: Always
          name: meshery
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: meshery-server
        serviceAccountName: meshery-server
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-01-09T22:47:49Z"
      lastUpdateTime: "2025-01-09T22:47:49Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-01-09T22:47:27Z"
      lastUpdateTime: "2025-01-09T22:47:49Z"
      message: ReplicaSet "meshery-6487b554bd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meshery/component-type: management-plane
    creationTimestamp: "2025-01-09T22:47:50Z"
    generation: 1
    labels:
      app: meshery
      component: meshsync
    name: meshery-meshsync
    namespace: meshery
    ownerReferences:
    - apiVersion: meshery.layer5.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: MeshSync
      name: meshery-meshsync
      uid: 635be6ad-6dd0-4d80-9453-1c0bd8ce4e15
    resourceVersion: "62656785"
    uid: 3dfa2790-7456-4bd8-bbed-a77a8628f397
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: meshery
        component: meshsync
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          meshery/component-type: management-plane
        creationTimestamp: null
        labels:
          app: meshery
          component: meshsync
        name: meshery-meshsync
      spec:
        containers:
        - command:
          - ./meshery-meshsync
          - --broker-url
          - $(BROKER_URL)
          env:
          - name: BROKER_URL
            value: 10.109.58.0:4222
          image: layer5/meshsync:stable-latest
          imagePullPolicy: Always
          name: meshsync
          ports:
          - containerPort: 11000
            hostPort: 11000
            name: client
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: meshery-operator
        serviceAccountName: meshery-operator
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-01-09T22:47:53Z"
      lastUpdateTime: "2025-01-09T22:47:53Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-01-09T22:47:50Z"
      lastUpdateTime: "2025-01-09T22:47:53Z"
      message: ReplicaSet "meshery-meshsync-59747c798d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: meshery-operator
      meta.helm.sh/release-namespace: meshery
    creationTimestamp: "2025-01-09T22:47:28Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: meshery-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: meshery-operator
      app.kubernetes.io/version: v0.8.8
      helm.sh/chart: meshery-operator-v0.8.8
    name: meshery-operator
    namespace: meshery
    resourceVersion: "62656723"
    uid: dae967d3-37ca-466d-940d-29d427347eca
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: meshery-operator
        app.kubernetes.io/name: meshery-operator
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: meshery-operator
          app.kubernetes.io/name: meshery-operator
      spec:
        containers:
        - args:
          - --metrics-addr=127.0.0.1:8080
          - --enable-leader-election
          command:
          - /manager
          image: layer5/meshery-operator:stable-latest
          imagePullPolicy: Always
          name: manager
          ports:
          - containerPort: 9443
            name: server
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - args:
          - --secure-listen-address=0.0.0.0:8443
          - --upstream=http://127.0.0.1:8080/
          - --logtostderr=false
          - --v=10
          image: gcr.io/kubebuilder/kube-rbac-proxy:v0.16.0
          imagePullPolicy: Always
          name: kube-rbac-proxy
          ports:
          - containerPort: 8443
            name: https
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: meshery-operator
        serviceAccountName: meshery-operator
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-01-09T22:47:50Z"
      lastUpdateTime: "2025-01-09T22:47:50Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-01-09T22:47:28Z"
      lastUpdateTime: "2025-01-09T22:47:50Z"
      message: ReplicaSet "meshery-operator-7c77d9b57f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: metallb
      meta.helm.sh/release-namespace: metallb
    creationTimestamp: "2024-04-01T04:00:06Z"
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: metallb
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: metallb
      app.kubernetes.io/version: v0.14.4
      helm.sh/chart: metallb-0.14.4
    name: metallb-controller
    namespace: metallb
    resourceVersion: "47870707"
    uid: db5ab78c-d6aa-4722-a585-52d637bfe0c8
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: metallb
        app.kubernetes.io/name: metallb
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: metallb
          app.kubernetes.io/name: metallb
      spec:
        containers:
        - args:
          - --port=7472
          - --log-level=info
          - --tls-min-version=VersionTLS12
          env:
          - name: METALLB_ML_SECRET_NAME
            value: metallb-memberlist
          - name: METALLB_DEPLOYMENT
            value: metallb-controller
          - name: METALLB_BGP_TYPE
            value: frr
          image: quay.io/metallb/controller:v0.14.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: monitoring
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 7472
            name: monitoring
            protocol: TCP
          - containerPort: 9443
            name: webhook-server
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: monitoring
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/k8s-webhook-server/serving-certs
            name: cert
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: metallb-controller
        serviceAccountName: metallb-controller
        terminationGracePeriodSeconds: 0
        volumes:
        - name: cert
          secret:
            defaultMode: 420
            secretName: metallb-webhook-cert
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-04-01T04:00:06Z"
      lastUpdateTime: "2024-04-01T04:00:17Z"
      message: ReplicaSet "metallb-controller-c65987fdb" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-11-06T05:50:54Z"
      lastUpdateTime: "2024-11-06T05:50:54Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: layer5
      meta.helm.sh/release-namespace: prod-cloud
    creationTimestamp: "2024-11-26T08:56:06Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: hydra
      app.kubernetes.io/version: v1.11.8
      helm.sh/chart: hydra-0.24.2
    name: layer5-hydra
    namespace: prod-cloud
    resourceVersion: "53125608"
    uid: c65f9e73-4b3b-4e65-874e-45f743e37b2e
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: layer5
        app.kubernetes.io/name: hydra
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/hydra-config: c52870726676f7ba3aa19fdf34f8fa1f8f266ed0b1750aa23d5aa00c10e68bf3
          checksum/hydra-secrets: e3d519a3a711383641afd27a94d503b8a4683060923bfda6faa97d805718ed36
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: layer5
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: hydra
          app.kubernetes.io/version: v1.11.8
          helm.sh/chart: hydra-0.24.2
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - serve
          - all
          - --dangerous-force-http
          - --config
          - /etc/config/hydra.yaml
          command:
          - hydra
          env:
          - name: URLS_SELF_ISSUER
            value: https://cloud.layer5.io/hydra
          - name: DSN
            valueFrom:
              secretKeyRef:
                key: dsn
                name: layer5-hydra
          - name: SECRETS_SYSTEM
            valueFrom:
              secretKeyRef:
                key: secretsSystem
                name: layer5-hydra
          - name: SECRETS_COOKIE
            valueFrom:
              secretKeyRef:
                key: secretsCookie
                name: layer5-hydra
          image: oryd/hydra:v1.11.8
          imagePullPolicy: IfNotPresent
          lifecycle: {}
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health/alive
              port: http-admin
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: hydra
          ports:
          - containerPort: 4444
            name: http-public
            protocol: TCP
          - containerPort: 4445
            name: http-admin
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /health/ready
              port: http-admin
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 100
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: hydra-config-volume
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: layer5-hydra
        serviceAccountName: layer5-hydra
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: layer5-hydra
          name: hydra-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-11-26T08:56:37Z"
      lastUpdateTime: "2024-11-26T08:56:37Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-11-26T08:56:06Z"
      lastUpdateTime: "2024-11-26T08:56:37Z"
      message: ReplicaSet "layer5-hydra-7854b8559d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: layer5
      meta.helm.sh/release-namespace: prod-cloud
    creationTimestamp: "2024-11-26T08:56:06Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: hydra-maester
      app.kubernetes.io/version: v0.0.23
      helm.sh/chart: hydra-maester-0.24.2
    name: layer5-hydra-maester
    namespace: prod-cloud
    resourceVersion: "53125523"
    uid: 7329a0df-cae7-4c50-a90c-abbba346d1c3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: layer5
        app.kubernetes.io/name: layer5-hydra-maester
        control-plane: controller-manager
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: layer5
          app.kubernetes.io/name: layer5-hydra-maester
          control-plane: controller-manager
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --metrics-addr=127.0.0.1:8080
          - --hydra-url=http://layer5-hydra-admin
          - --hydra-port=4445
          command:
          - /manager
          image: oryd/hydra-maester:v0.0.25
          imagePullPolicy: IfNotPresent
          name: hydra-maester
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: layer5-hydra-maester-account
        serviceAccountName: layer5-hydra-maester-account
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-11-26T08:56:08Z"
      lastUpdateTime: "2024-11-26T08:56:08Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-11-26T08:56:06Z"
      lastUpdateTime: "2024-11-26T08:56:08Z"
      message: ReplicaSet "layer5-hydra-maester-5c558b9b7b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "23"
      meta.helm.sh/release-name: layer5
      meta.helm.sh/release-namespace: prod-cloud
    creationTimestamp: "2024-11-26T08:56:06Z"
    generation: 23
    labels:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kratos
      app.kubernetes.io/version: v1.0.0
      helm.sh/chart: kratos-0.39.1
    name: layer5-kratos
    namespace: prod-cloud
    resourceVersion: "62021196"
    uid: 2f4d6ea5-fe54-429a-b6d9-8841f4429753
  spec:
    progressDeadlineSeconds: 3600
    replicas: 1
    revisionHistoryLimit: 5
    selector:
      matchLabels:
        app.kubernetes.io/instance: layer5
        app.kubernetes.io/name: kratos
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/kratos-config: cb52a43523bc090ee27026eac852b8ec94894047102722d74af5a2706eceec68
          checksum/kratos-secrets: 01c4ccd7b5521b90e3f3f39e7d0df69865c582a07b9121131b474e18f69efd9f
          checksum/kratos-templates: 06dc42590b5035ee28993ffdf0195d39cca757fd9ebb428f06d4935bf3c03af6
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: layer5
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kratos
          app.kubernetes.io/version: v1.0.0
          helm.sh/chart: kratos-0.39.1
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - serve
          - all
          - --config
          - /etc/config/kratos.yaml
          command:
          - kratos
          env:
          - name: DSN
            valueFrom:
              secretKeyRef:
                key: dsn
                name: layer5-kratos
          - name: SECRETS_DEFAULT
            valueFrom:
              secretKeyRef:
                key: secretsDefault
                name: layer5-kratos
                optional: true
          - name: SECRETS_COOKIE
            valueFrom:
              secretKeyRef:
                key: secretsCookie
                name: layer5-kratos
                optional: true
          - name: SECRETS_CIPHER
            valueFrom:
              secretKeyRef:
                key: secretsCipher
                name: layer5-kratos
                optional: true
          - name: COURIER_SMTP_CONNECTION_URI
            valueFrom:
              secretKeyRef:
                key: smtpConnectionURI
                name: layer5-kratos
          image: oryd/kratos:v1.0.0
          imagePullPolicy: IfNotPresent
          lifecycle: {}
          livenessProbe:
            failureThreshold: 5
            httpGet:
              httpHeaders:
              - name: Host
                value: 127.0.0.1
              path: /admin/health/alive
              port: 4434
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: kratos
          ports:
          - containerPort: 4434
            name: http-admin
            protocol: TCP
          - containerPort: 4433
            name: http-public
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              httpHeaders:
              - name: Host
                value: 127.0.0.1
              path: /admin/health/ready
              port: 4434
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
            seLinuxOptions:
              level: s0:c123,c456
            seccompProfile:
              type: RuntimeDefault
          startupProbe:
            failureThreshold: 60
            httpGet:
              httpHeaders:
              - name: Host
                value: 127.0.0.1
              path: /admin/health/ready
              port: 4434
              scheme: HTTP
            periodSeconds: 1
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: kratos-config-volume
            readOnly: true
          - mountPath: /conf/courier-templates/recovery/valid
            name: kratos-template-recovery-valid-volume
            readOnly: true
          - mountPath: /conf/courier-templates/verification/valid
            name: kratos-template-verification-valid-volume
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: layer5-kratos
        serviceAccountName: layer5-kratos
        terminationGracePeriodSeconds: 60
        volumes:
        - configMap:
            defaultMode: 420
            name: layer5-kratos-config
          name: kratos-config-volume
        - configMap:
            defaultMode: 420
            name: layer5-kratos-template-recovery-valid
          name: kratos-template-recovery-valid-volume
        - configMap:
            defaultMode: 420
            name: layer5-kratos-template-verification-valid
          name: kratos-template-verification-valid-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-12-06T15:13:23Z"
      lastUpdateTime: "2024-12-06T15:13:23Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-11-26T08:56:06Z"
      lastUpdateTime: "2025-01-06T20:42:13Z"
      message: ReplicaSet "layer5-kratos-d67fc5d44" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 23
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "23"
      meta.helm.sh/release-name: layer5
      meta.helm.sh/release-namespace: prod-cloud
    creationTimestamp: "2024-11-26T08:56:06Z"
    generation: 23
    labels:
      app.kubernetes.io/instance: layer5
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: layer5-cloud
      app.kubernetes.io/version: 0.7.13
      meshery-cloud.sh/chart: layer5-cloud-0.1.0
    name: meshery-cloud
    namespace: prod-cloud
    resourceVersion: "63490504"
    uid: 78fe884e-7bbd-42ba-b765-f1a50e6d2d69
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: layer5
        app.kubernetes.io/name: layer5-cloud
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: layer5
          app.kubernetes.io/name: layer5-cloud
        name: meshery-cloud
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: meshery-cloud-cm
          - secretRef:
              name: meshery-cloud-secret
          image: layer5/meshery-cloud:production-v0.8.119
          imagePullPolicy: IfNotPresent
          name: layer5-cloud
          ports:
          - containerPort: 9876
            name: http
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: github-app-key
          secret:
            defaultMode: 420
            secretName: meshery-cloud
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-11-26T08:56:06Z"
      lastUpdateTime: "2025-01-06T20:42:10Z"
      message: ReplicaSet "meshery-cloud-6d4b8bdb67" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-01-13T16:37:34Z"
      lastUpdateTime: "2025-01-13T16:37:34Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 23
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prod-monitoring
    creationTimestamp: "2024-11-22T07:18:43Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 11.3.0
      helm.sh/chart: grafana-8.6.0
    name: prometheus-grafana
    namespace: prod-monitoring
    resourceVersion: "52150513"
    uid: 72fdb95d-81e3-42f7-bea0-650c0356ff49
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: grafana
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: grafana
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.28.0
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.28.0
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:11.3.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: prometheus-grafana
        serviceAccountName: prometheus-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: prometheus-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-11-22T07:19:04Z"
      lastUpdateTime: "2024-11-22T07:19:04Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-11-22T07:18:43Z"
      lastUpdateTime: "2024-11-22T07:19:04Z"
      message: ReplicaSet "prometheus-grafana-69f9ccfd8d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prod-monitoring
    creationTimestamp: "2024-11-22T07:18:43Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 66.2.1
      chart: kube-prometheus-stack-66.2.1
      heritage: Helm
      release: prometheus
    name: prometheus-kube-prometheus-operator
    namespace: prod-monitoring
    resourceVersion: "52150472"
    uid: f42d7971-5305-4332-848c-0dcb2172e3c6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        release: prometheus
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/component: prometheus-operator
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 66.2.1
          chart: kube-prometheus-stack-66.2.1
          heritage: Helm
          release: prometheus
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --kubelet-service=kube-system/prometheus-kube-prometheus-kubelet
          - --kubelet-endpoints=true
          - --kubelet-endpointslice=false
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.78.1
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.36.1
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.78.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-prometheus-operator
        serviceAccountName: prometheus-kube-prometheus-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: prometheus-kube-prometheus-admission
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-11-22T07:18:56Z"
      lastUpdateTime: "2024-11-22T07:18:56Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-11-22T07:18:43Z"
      lastUpdateTime: "2024-11-22T07:18:56Z"
      message: ReplicaSet "prometheus-kube-prometheus-operator-6d87886878" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prod-monitoring
    creationTimestamp: "2024-11-22T07:18:43Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.27.0
      release: prometheus
    name: prometheus-kube-state-metrics
    namespace: prod-monitoring
    resourceVersion: "52150411"
    uid: 9f41992d-634a-4bb1-9cbc-2a4908b74c29
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.27.0
          release: prometheus
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-11-22T07:18:53Z"
      lastUpdateTime: "2024-11-22T07:18:53Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-11-22T07:18:43Z"
      lastUpdateTime: "2024-11-22T07:18:53Z"
      message: ReplicaSet "prometheus-kube-state-metrics-66f5694654" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2024-04-01T03:46:19Z"
    generation: 1
    labels:
      k8s-app: kube-proxy
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "402"
    uid: e13a2b45-a00d-48fb-b126-6712c3236b8a
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-proxy
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-proxy
      spec:
        containers:
        - command:
          - /usr/local/bin/kube-proxy
          - --config=/var/lib/kube-proxy/config.conf
          - --hostname-override=$(NODE_NAME)
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: registry.k8s.io/kube-proxy:v1.29.3
          imagePullPolicy: IfNotPresent
          name: kube-proxy
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kube-proxy
            name: kube-proxy
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-proxy
        serviceAccountName: kube-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-proxy
          name: kube-proxy
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"name":"weave-net"},"name":"weave-net","namespace":"kube-system"},"spec":{"minReadySeconds":5,"selector":{"matchLabels":{"name":"weave-net"}},"template":{"metadata":{"labels":{"name":"weave-net"}},"spec":{"containers":[{"command":["/home/weave/launch.sh"],"env":[{"name":"INIT_CONTAINER","value":"true"},{"name":"HOSTNAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"spec.nodeName"}}},{"name":"IPALLOC_RANGE","value":"192.168.0.0/16"}],"image":"weaveworks/weave-kube:latest","imagePullPolicy":"Always","name":"weave","readinessProbe":{"httpGet":{"host":"127.0.0.1","path":"/status","port":6784}},"resources":{"requests":{"cpu":"50m"}},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/weavedb","name":"weavedb"},{"mountPath":"/host/var/lib/dbus","name":"dbus","readOnly":true},{"mountPath":"/host/etc/machine-id","name":"cni-machine-id","readOnly":true},{"mountPath":"/run/xtables.lock","name":"xtables-lock","readOnly":false}]},{"env":[{"name":"HOSTNAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"spec.nodeName"}}}],"image":"weaveworks/weave-npc:latest","imagePullPolicy":"Always","name":"weave-npc","resources":{"requests":{"cpu":"50m"}},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/run/xtables.lock","name":"xtables-lock","readOnly":false}]}],"dnsPolicy":"ClusterFirstWithHostNet","hostNetwork":true,"hostPID":false,"initContainers":[{"command":["/home/weave/init.sh"],"env":null,"image":"weaveworks/weave-kube:latest","imagePullPolicy":"Always","name":"weave-init","securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/host/opt","name":"cni-bin"},{"mountPath":"/host/home","name":"cni-bin2"},{"mountPath":"/host/etc","name":"cni-conf"},{"mountPath":"/lib/modules","name":"lib-modules"},{"mountPath":"/run/xtables.lock","name":"xtables-lock","readOnly":false}]}],"priorityClassName":"system-node-critical","restartPolicy":"Always","securityContext":{"seLinuxOptions":{}},"serviceAccountName":"weave-net","tolerations":[{"effect":"NoSchedule","operator":"Exists"},{"effect":"NoExecute","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/var/lib/weave"},"name":"weavedb"},{"hostPath":{"path":"/opt"},"name":"cni-bin"},{"hostPath":{"path":"/home"},"name":"cni-bin2"},{"hostPath":{"path":"/etc"},"name":"cni-conf"},{"hostPath":{"path":"/etc/machine-id"},"name":"cni-machine-id"},{"hostPath":{"path":"/var/lib/dbus"},"name":"dbus"},{"hostPath":{"path":"/lib/modules"},"name":"lib-modules"},{"hostPath":{"path":"/run/xtables.lock","type":"FileOrCreate"},"name":"xtables-lock"}]}},"updateStrategy":{"type":"RollingUpdate"}}}
    creationTimestamp: "2024-04-01T03:55:12Z"
    generation: 1
    labels:
      name: weave-net
    name: weave-net
    namespace: kube-system
    resourceVersion: "47870895"
    uid: 532bcd57-b9e9-4ce8-882f-82fba72dac48
  spec:
    minReadySeconds: 5
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        name: weave-net
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: weave-net
      spec:
        containers:
        - command:
          - /home/weave/launch.sh
          env:
          - name: INIT_CONTAINER
            value: "true"
          - name: HOSTNAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: IPALLOC_RANGE
            value: 192.168.0.0/16
          image: weaveworks/weave-kube:latest
          imagePullPolicy: Always
          name: weave
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              path: /status
              port: 6784
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 50m
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /weavedb
            name: weavedb
          - mountPath: /host/var/lib/dbus
            name: dbus
            readOnly: true
          - mountPath: /host/etc/machine-id
            name: cni-machine-id
            readOnly: true
          - mountPath: /run/xtables.lock
            name: xtables-lock
        - env:
          - name: HOSTNAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: weaveworks/weave-npc:latest
          imagePullPolicy: Always
          name: weave-npc
          resources:
            requests:
              cpu: 50m
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /run/xtables.lock
            name: xtables-lock
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        initContainers:
        - command:
          - /home/weave/init.sh
          image: weaveworks/weave-kube:latest
          imagePullPolicy: Always
          name: weave-init
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/opt
            name: cni-bin
          - mountPath: /host/home
            name: cni-bin2
          - mountPath: /host/etc
            name: cni-conf
          - mountPath: /lib/modules
            name: lib-modules
          - mountPath: /run/xtables.lock
            name: xtables-lock
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          seLinuxOptions: {}
        serviceAccount: weave-net
        serviceAccountName: weave-net
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
        volumes:
        - hostPath:
            path: /var/lib/weave
            type: ""
          name: weavedb
        - hostPath:
            path: /opt
            type: ""
          name: cni-bin
        - hostPath:
            path: /home
            type: ""
          name: cni-bin2
        - hostPath:
            path: /etc
            type: ""
          name: cni-conf
        - hostPath:
            path: /etc/machine-id
            type: ""
          name: cni-machine-id
        - hostPath:
            path: /var/lib/dbus
            type: ""
          name: dbus
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: metallb
      meta.helm.sh/release-namespace: metallb
    creationTimestamp: "2024-04-01T04:00:06Z"
    generation: 1
    labels:
      app.kubernetes.io/component: speaker
      app.kubernetes.io/instance: metallb
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: metallb
      app.kubernetes.io/version: v0.14.4
      helm.sh/chart: metallb-0.14.4
    name: metallb-speaker
    namespace: metallb
    resourceVersion: "47870923"
    uid: 8a96e7a5-a2e3-491d-94f9-4a3b7202fa5a
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: speaker
        app.kubernetes.io/instance: metallb
        app.kubernetes.io/name: metallb
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: speaker
          app.kubernetes.io/instance: metallb
          app.kubernetes.io/name: metallb
      spec:
        containers:
        - args:
          - --port=7472
          - --log-level=info
          env:
          - name: METALLB_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: METALLB_HOST
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.hostIP
          - name: METALLB_ML_BIND_ADDR
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: METALLB_ML_LABELS
            value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker
          - name: METALLB_ML_BIND_PORT
            value: "7946"
          - name: METALLB_ML_SECRET_KEY_PATH
            value: /etc/ml_secret_key
          - name: FRR_CONFIG_FILE
            value: /etc/frr_reloader/frr.conf
          - name: FRR_RELOADER_PID_FILE
            value: /etc/frr_reloader/reloader.pid
          - name: METALLB_BGP_TYPE
            value: frr
          image: quay.io/metallb/speaker:v0.14.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: monitoring
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: speaker
          ports:
          - containerPort: 7472
            name: monitoring
            protocol: TCP
          - containerPort: 7946
            name: memberlist-tcp
            protocol: TCP
          - containerPort: 7946
            name: memberlist-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: monitoring
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_RAW
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ml_secret_key
            name: memberlist
          - mountPath: /etc/frr_reloader
            name: reloader
          - mountPath: /etc/metallb
            name: metallb-excludel2
        - command:
          - /bin/sh
          - -c
          - |
            /sbin/tini -- /usr/lib/frr/docker-start &
            attempts=0
            until [[ -f /etc/frr/frr.log || $attempts -eq 60 ]]; do
              sleep 1
              attempts=$(( $attempts + 1 ))
            done
            tail -f /etc/frr/frr.log
          env:
          - name: TINI_SUBREAPER
            value: "true"
          image: quay.io/frrouting/frr:9.0.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: livez
              port: 7473
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: frr
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
              - NET_RAW
              - SYS_ADMIN
              - NET_BIND_SERVICE
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /livez
              port: 7473
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/run/frr
            name: frr-sockets
          - mountPath: /etc/frr
            name: frr-conf
        - command:
          - /etc/frr_reloader/frr-reloader.sh
          image: quay.io/frrouting/frr:9.0.2
          imagePullPolicy: IfNotPresent
          name: reloader
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/run/frr
            name: frr-sockets
          - mountPath: /etc/frr
            name: frr-conf
          - mountPath: /etc/frr_reloader
            name: reloader
        - args:
          - --metrics-port=7473
          command:
          - /etc/frr_metrics/frr-metrics
          image: quay.io/frrouting/frr:9.0.2
          imagePullPolicy: IfNotPresent
          name: frr-metrics
          ports:
          - containerPort: 7473
            name: monitoring
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/run/frr
            name: frr-sockets
          - mountPath: /etc/frr
            name: frr-conf
          - mountPath: /etc/frr_metrics
            name: metrics
        dnsPolicy: ClusterFirst
        hostNetwork: true
        initContainers:
        - command:
          - /bin/sh
          - -c
          - cp -rLf /tmp/frr/* /etc/frr/
          image: quay.io/frrouting/frr:9.0.2
          imagePullPolicy: IfNotPresent
          name: cp-frr-files
          resources: {}
          securityContext:
            runAsGroup: 101
            runAsUser: 100
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/frr
            name: frr-startup
          - mountPath: /etc/frr
            name: frr-conf
        - command:
          - /bin/sh
          - -c
          - cp -f /frr-reloader.sh /etc/frr_reloader/
          image: quay.io/metallb/speaker:v0.14.4
          imagePullPolicy: IfNotPresent
          name: cp-reloader
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/frr_reloader
            name: reloader
        - command:
          - /bin/sh
          - -c
          - cp -f /frr-metrics /etc/frr_metrics/
          image: quay.io/metallb/speaker:v0.14.4
          imagePullPolicy: IfNotPresent
          name: cp-metrics
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/frr_metrics
            name: metrics
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metallb-speaker
        serviceAccountName: metallb-speaker
        shareProcessNamespace: true
        terminationGracePeriodSeconds: 0
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        volumes:
        - name: memberlist
          secret:
            defaultMode: 420
            secretName: metallb-memberlist
        - configMap:
            defaultMode: 256
            name: metallb-excludel2
          name: metallb-excludel2
        - emptyDir: {}
          name: frr-sockets
        - configMap:
            defaultMode: 420
            name: metallb-frr-startup
          name: frr-startup
        - emptyDir: {}
          name: frr-conf
        - emptyDir: {}
          name: reloader
        - emptyDir: {}
          name: metrics
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prod-monitoring
    creationTimestamp: "2024-11-22T07:18:43Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      helm.sh/chart: prometheus-node-exporter-4.42.0
      release: prometheus
    name: prometheus-prometheus-node-exporter
    namespace: prod-monitoring
    resourceVersion: "52150356"
    uid: d85a33c8-0883-4216-9fab-33511603960a
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-node-exporter
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-node-exporter
          app.kubernetes.io/part-of: prometheus-node-exporter
          app.kubernetes.io/version: 1.8.2
          helm.sh/chart: prometheus-node-exporter-4.42.0
          jobLabel: node-exporter
          release: prometheus
      spec:
        automountServiceAccountToken: false
        containers:
        - args:
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --path.rootfs=/host/root
          - --path.udev.data=/host/root/run/udev/data
          - --web.listen-address=[$(HOST_IP)]:9100
          - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
          - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
          env:
          - name: HOST_IP
            value: 0.0.0.0
          image: quay.io/prometheus/node-exporter:v1.8.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: node-exporter
          ports:
          - containerPort: 9100
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        hostPID: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-node-exporter
        serviceAccountName: prometheus-prometheus-node-exporter
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2024-04-01T04:29:32Z"
    labels:
      kubernetes.io/metadata.name: cert-manager
      name: cert-manager
    name: cert-manager
    resourceVersion: "4320"
    uid: 3cb77293-d185-42e9-8d47-0062b85d9f1b
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    annotations:
      component.meshery.io/id: 3fc194ac-5fa4-49ba-83d3-8a34a06bf99c
      design.meshery.io/version: 0.0.10
    creationTimestamp: "2024-04-01T03:46:16Z"
    labels:
      design.meshery.io/id: c80a3f4e-e764-40b5-84ab-2c9aa0776ffb
      kubernetes.io/metadata.name: default
    name: default
    resourceVersion: "62836325"
    uid: 864202c9-efdd-4db6-9ffa-2e82e138ee6f
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2024-04-01T04:10:26Z"
    labels:
      kubernetes.io/metadata.name: emissary
    name: emissary
    resourceVersion: "2482"
    uid: 5fcecc7d-7312-4806-b622-d76b8f24ec01
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Namespace","metadata":{"annotations":{},"name":"emissary-system"}}
    creationTimestamp: "2024-10-01T15:29:56Z"
    labels:
      kubernetes.io/metadata.name: emissary-system
    name: emissary-system
    resourceVersion: "38668478"
    uid: b9bb71e4-5eff-4a62-82c2-4e69cfb7e921
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Namespace","metadata":{"annotations":{},"labels":{"app.kubernetes.io/instance":"ingress-nginx","app.kubernetes.io/name":"ingress-nginx"},"name":"ingress-nginx"}}
    creationTimestamp: "2024-11-09T04:07:12Z"
    labels:
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
      kubernetes.io/metadata.name: ingress-nginx
    name: ingress-nginx
    resourceVersion: "48624500"
    uid: 1bf53e4f-7a8e-487d-9bc0-9bf18bbee224
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2024-04-01T03:46:16Z"
    labels:
      kubernetes.io/metadata.name: kube-node-lease
    name: kube-node-lease
    resourceVersion: "34"
    uid: c530bd0f-2635-4173-8098-96a9f91b74be
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2024-04-01T03:46:16Z"
    labels:
      kubernetes.io/metadata.name: kube-public
    name: kube-public
    resourceVersion: "23"
    uid: 41e1f5e9-d543-495c-ae83-a6daf8a90615
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2024-04-01T03:46:16Z"
    labels:
      kubernetes.io/metadata.name: kube-system
    name: kube-system
    resourceVersion: "12"
    uid: 691ac523-575f-4af4-aacd-390a7b705ff1
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2024-10-04T04:59:51Z"
    deletionTimestamp: "2024-11-04T01:33:30Z"
    labels:
      kubernetes.io/metadata.name: kubeapps
    name: kubeapps
    resourceVersion: "49338159"
    uid: 83e1f0ab-23f1-4a00-a97f-a1fd4dfc9344
  spec:
    finalizers:
    - kubernetes
  status:
    conditions:
    - lastTransitionTime: "2024-11-11T19:36:21Z"
      message: All resources successfully discovered
      reason: ResourcesDiscovered
      status: "False"
      type: NamespaceDeletionDiscoveryFailure
    - lastTransitionTime: "2024-11-04T01:33:36Z"
      message: All legacy kube types successfully parsed
      reason: ParsedGroupVersions
      status: "False"
      type: NamespaceDeletionGroupVersionParsingFailure
    - lastTransitionTime: "2024-11-04T01:33:36Z"
      message: All content successfully deleted, may be waiting on finalization
      reason: ContentDeleted
      status: "False"
      type: NamespaceDeletionContentFailure
    - lastTransitionTime: "2024-11-04T01:33:36Z"
      message: 'Some resources are remaining: apprepositories.kubeapps.com has 1 resource
        instances'
      reason: SomeResourcesRemain
      status: "True"
      type: NamespaceContentRemaining
    - lastTransitionTime: "2024-11-04T01:33:36Z"
      message: 'Some content in the namespace has finalizers remaining: apprepositories.kubeapps.com/apprepo-cleanup-finalizer
        in 1 resource instances'
      reason: SomeFinalizersRemain
      status: "True"
      type: NamespaceFinalizersRemaining
    phase: Terminating
- apiVersion: v1
  kind: Namespace
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Namespace","metadata":{"annotations":{},"name":"local-path-storage"}}
    creationTimestamp: "2024-04-29T12:30:38Z"
    labels:
      kubernetes.io/metadata.name: local-path-storage
    name: local-path-storage
    resourceVersion: "5900357"
    uid: 39d8a425-3e8f-43ea-a85f-ef467242701f
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2025-01-09T22:47:27Z"
    labels:
      kubernetes.io/metadata.name: meshery
      name: meshery
    name: meshery
    resourceVersion: "62656593"
    uid: c8828c33-7133-44b0-9f62-c75f4ecebfbb
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2024-09-03T17:17:44Z"
    labels:
      kubernetes.io/metadata.name: meshery-extensions
    name: meshery-extensions
    resourceVersion: "32572614"
    uid: c2f99248-4081-481e-be6e-cbf74176779b
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2024-04-01T04:00:06Z"
    labels:
      kubernetes.io/metadata.name: metallb
      name: metallb
    name: metallb
    resourceVersion: "1562"
    uid: 735540fc-a382-4bf3-8709-e31e478bf907
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2024-11-26T00:47:59Z"
    labels:
      kubernetes.io/metadata.name: pgdump
    name: pgdump
    resourceVersion: "53062697"
    uid: 69525ce4-8f23-470b-8c68-37a23497f546
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2024-11-11T11:49:16Z"
    labels:
      kubernetes.io/metadata.name: prod-cloud
    name: prod-cloud
    resourceVersion: "49250161"
    uid: 40c56e86-f056-4cde-8bf5-e1ab06a026dc
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2024-11-26T07:44:19Z"
    labels:
      kubernetes.io/metadata.name: prod-db
    name: prod-db
    resourceVersion: "53116311"
    uid: 2b53c0e4-675f-4014-abf8-e4b92ca9d7a0
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2024-11-22T07:18:37Z"
    labels:
      kubernetes.io/metadata.name: prod-monitoring
      name: prod-monitoring
    name: prod-monitoring
    resourceVersion: "52150063"
    uid: 5e994226-5cc3-4a5a-a226-b77a90cf182c
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2024-05-16T06:42:59Z"
    labels:
      kubernetes.io/metadata.name: tc-system
    name: tc-system
    resourceVersion: "9039832"
    uid: 7c2332a3-2b15-4812-8eff-b431d22ede39
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    annotations:
      component.meshery.io/id: 0025dd47-3eb8-466e-982a-4e57134595ec
      design.meshery.io/version: 0.0.1
    creationTimestamp: "2024-12-19T12:53:07Z"
    labels:
      design.meshery.io/id: ebe98a52-a5fe-4757-bea9-35e79da96ac0
      kubernetes.io/metadata.name: test-namespace
    name: test-namespace
    resourceVersion: "58622003"
    uid: 6e5a7347-146f-45e5-aee6-9ed344429d8e
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
kind: List
metadata:
  resourceVersion: ""
